{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP423: Assignment 1\n",
    "\n",
    "Group Number: 5\n",
    "\n",
    "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGHT NEED TO RUN THIS FOR THE NLTK \n",
    "# pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kayleighhabib/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kayleighhabib/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : Text Preprocessing \n",
    "a. Convert all text to lowercase\n",
    "\n",
    "b. Tokenize the text using NLTK\n",
    "\n",
    "c. Remove stop words using NLTK\n",
    "\n",
    "d. Exclude special characters except alphanumeric characters\n",
    "\n",
    "e. Eliminate singly occurring characters\n",
    "\n",
    "f. Create a set of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(file):\n",
    "\n",
    "    # reading the file line by line\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # a. converting to lowercase\n",
    "    lines = [x.lower() for x in lines]\n",
    "\n",
    "    # join all the lines into one string\n",
    "    data = \"\".join(lines)\n",
    "    \n",
    "    # b. using the tokenization package to tokenize the words\n",
    "    tokenization = word_tokenize(data)\n",
    "\n",
    "    # remove duplicates - less words to do the next steps with\n",
    "    newtoken = list(dict.fromkeys(tokenization))\n",
    "    \n",
    "    # c. remove any stop words from the NLTK package\n",
    "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
    "\n",
    "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
    "    stop_words = []\n",
    "    for word in newtoken:\n",
    "            if word not in stop_words_lib:\n",
    "                 stop_words.append(word)\n",
    "    \n",
    "    # print(stop_words)\n",
    "\n",
    "    # d. Remove special characters\n",
    "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
    "    alphanumerictxt = []\n",
    "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
    "    # strips the empty string afterwards\n",
    "    for i in stop_words: \n",
    "        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n",
    "\n",
    "    # e. eliminating any characters of length 1\n",
    "    updated_list = []\n",
    "    \n",
    "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
    "    for i in alphanumerictxt:\n",
    "        if len(i)>1:\n",
    "            updated_list.append(i)\n",
    "\n",
    "    # after all the other steps are completed check again to make sure no duplicated values\n",
    "    final_list = list(dict.fromkeys(updated_list))\n",
    "\n",
    "    #f\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data files\n",
    "# takes the names of the files in the folder and creates a list\n",
    "file_names = os.listdir(\"./data\") \n",
    "# file_names\n",
    "\n",
    "# file id (key)\n",
    "counter = 1\n",
    "\n",
    "#dictionary1 - translates the ID which belong to the file\n",
    "dict_words = {}\n",
    "# dictionaty2 - file dictionary (id to file name)\n",
    "dict_files = {}\n",
    "\n",
    "# goes through each file in the list of file names\n",
    "for i in file_names:\n",
    "    # gets the working directory (this may be different for everyone)\n",
    "    file_path = os.getcwd() + \"/data/\" + i \n",
    "    # opens the file in reading mode\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
    "    # calls the function written for question 1 of the assignment\n",
    "    words = text_preprocessing(file)\n",
    "    \n",
    "    # updates the dictionaries\n",
    "    dict_words[counter] = words # list of words in this file\n",
    "    dict_files[counter] = i # list of all the file names\n",
    "\n",
    "    # increment the counter\n",
    "    counter+=1\n",
    "\n",
    "    # closes the file\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : Inverted Index Implementation\n",
    "Implement an inverted index data structure for the preprocessed dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree / hash table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Query Support\n",
    "Support the following queries, where x and y would be taken as input from the user\n",
    "\n",
    "1. x OR y\n",
    "\n",
    "2. x AND y\n",
    "\n",
    "3. x AND NOT y\n",
    "\n",
    "4. x OR NOT y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: System Evaluation\n",
    "Evaluate your system against the set of provided queries. \n",
    "\n",
    "1. Query #1\n",
    "\n",
    "- Input Sentence: \"lion stood thoughtfully for a moment\"\n",
    "\n",
    "- Input operation sequence: [OR, OR, OR]\n",
    "\n",
    "- Expected preprocessed query: \"lion OR stood OR thoughtfully OR moment\"\n",
    "\n",
    "2. Query #2\n",
    "\n",
    "- Input Sentence: \"telephone, paved, roads\"\n",
    "\n",
    "- Input operation sequence: [OR NOT, AND NOT]\n",
    "\n",
    "- Expected preprocessed query: \"telephone OR NOT paved AND NOT roads\n",
    "\n",
    "\n",
    "Output should include:\n",
    "- The number of documents retrieved\n",
    "\n",
    "- The minimum number of total comparisons made (only for merging the algorithm)\n",
    "\n",
    "- The list of retrieved document names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the required print statements\n",
    "print(\"Number of matched documents: \")\n",
    "print(\"Minimum number of comparisons required\")\n",
    "print(\"List of retreived document names: \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
