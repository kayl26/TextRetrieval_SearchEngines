{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFj5zP--qYUN"
      },
      "source": [
        "# CP423: Assignment 1\n",
        "\n",
        "Group Number: 5\n",
        "\n",
        "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QOMJiPs5qYUP"
      },
      "outputs": [],
      "source": [
        "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
        "# pip install certifi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FHuZV0qFqYUQ"
      },
      "outputs": [],
      "source": [
        "import certifi\n",
        "import ssl\n",
        "import os\n",
        "\n",
        "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUAdJUSpqYUQ",
        "outputId": "2cdbf703-deab-43a8-e978-90826025f821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# inputs\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI_dT14qYUQ"
      },
      "source": [
        "## Question 1 : Text Preprocessing\n",
        "a. Convert all text to lowercase\n",
        "\n",
        "b. Tokenize the text using NLTK\n",
        "\n",
        "c. Remove stop words using NLTK\n",
        "\n",
        "d. Exclude special characters except alphanumeric characters\n",
        "\n",
        "e. Eliminate singly occurring characters\n",
        "\n",
        "f. Create a set of all the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rigjoVIdqYUR"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(file):\n",
        "\n",
        "    # reading the file line by line\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # a. converting to lowercase\n",
        "    lines = [x.lower() for x in lines]\n",
        "\n",
        "    # join all the lines into one string\n",
        "    data = \"\".join(lines)\n",
        "\n",
        "    # b. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "\n",
        "    # remove duplicates - less words to do the next steps with\n",
        "    newtoken = list(dict.fromkeys(tokenization))\n",
        "\n",
        "    # c. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in newtoken:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "    # print(stop_words)\n",
        "\n",
        "    # d. Remove special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
        "    alphanumerictxt = []\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in stop_words:\n",
        "        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n",
        "\n",
        "    # e. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in alphanumerictxt:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "    # after all the other steps are completed check again to make sure no duplicated values\n",
        "    final_list = list(dict.fromkeys(updated_list))\n",
        "\n",
        "    #f\n",
        "    return final_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "-9skAm3HqYUR",
        "outputId": "378d8e15-284d-4e40-b5e0-48c7c0e0d01c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'list' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-a3abe10a2202>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;31m# calls the function written for question 1 of the assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;31m# updates the dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-e499a3bb35f4>\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# remove duplicates - less words to do the next steps with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnewtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# c. remove any stop words from the NLTK package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ],
      "source": [
        "# import data files\n",
        "# takes the names of the files in the folder and creates a list\n",
        "file_names = os.listdir(\"./data\")\n",
        "# file_names\n",
        "\n",
        "# file id (key)\n",
        "counter = 1\n",
        "\n",
        "#dictionary1 - translates the ID which belong to the file\n",
        "dict_words = {}\n",
        "# dictionaty2 - file dictionary (id to file name)\n",
        "dict_files = {}\n",
        "\n",
        "# goes through each file in the list of file names\n",
        "for i in file_names:\n",
        "\n",
        "    # if counter <= 2: #for checking purposes - can remove before submitting\n",
        "\n",
        "      # gets the working directory (this may be different for everyone)\n",
        "      file_path = os.getcwd() + \"/data/\" + i\n",
        "      # opens the file in reading mode\n",
        "      file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
        "      # calls the function written for question 1 of the assignment\n",
        "      words = text_preprocessing(file)\n",
        "\n",
        "      # updates the dictionaries\n",
        "      dict_words[counter] = words # list of words in this file\n",
        "      dict_files[counter] = i # list of all the file names\n",
        "\n",
        "    # increment the counter\n",
        "      counter+=1\n",
        "\n",
        "    # closes the file\n",
        "      file.close()\n",
        "\n",
        "print(dict_words)\n",
        "print(dict_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auQGFmd1qYUR"
      },
      "source": [
        "## Question 2 : Inverted Index Implementation\n",
        "Implement an inverted index data structure for the preprocessed dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8fvqXm-KJ_qy"
      },
      "outputs": [],
      "source": [
        "class HashInvertedIndex:\n",
        "\n",
        "    #initialize hash table\n",
        "    def __init__(self, capacity=100):\n",
        "        self.capacity = capacity\n",
        "        self.table = [[] for _ in range(capacity)]\n",
        "        self.count = 0\n",
        "\n",
        "    #finding the corresponding value for the key\n",
        "    def _index(self, word):\n",
        "        index = hash(word) % self.capacity\n",
        "        return index\n",
        "\n",
        "    #add key-value pair to the hash table\n",
        "    def insert(self, key, value):\n",
        "\n",
        "        #compute the hash index for the word\n",
        "        hash_index = self._index(key)\n",
        "        for i, kv in enumerate(self.table[hash_index]):\n",
        "            k, v = kv\n",
        "            if k == key:\n",
        "                #add the value to the existing set of values\n",
        "                v.add(value)\n",
        "                #add the entry to the table\n",
        "                self.table[hash_index][i] = (k,v)\n",
        "                return\n",
        "        self.table[hash_index].append((key, {value}))\n",
        "\n",
        "    #retrieve the set of values for a given key, if not found, returns an empty set\n",
        "    def retrieve(self, key):\n",
        "        #compute the hash index for a given key\n",
        "        hash_index = self._index(key)\n",
        "        for k, v in self.table[hash_index]:\n",
        "            #if the key if found, teturn the set\n",
        "            if k == key:\n",
        "                return v\n",
        "        return set()\n",
        "\n",
        "    #for testing purposes - shows us the string representation of the hash tables\n",
        "    def __rep__(self):\n",
        "        return str(self.table)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jV-QPzJvJ_qy"
      },
      "outputs": [],
      "source": [
        "#build the inverted index\n",
        "def create_inverted_index():\n",
        "\n",
        "    inverted_index = HashInvertedIndex()\n",
        "\n",
        "## Kayleigh's code to evaluate functions\n",
        "\n",
        "    # import data files\n",
        "    # takes the names of the files in the folder and creates a list\n",
        "    file_names = os.listdir(\"./data\")\n",
        "    # file_names\n",
        "\n",
        "    # file id (key)\n",
        "    counter = 1\n",
        "\n",
        "    #dictionary1 - translates the ID which belong to the file\n",
        "    dict_words = {}\n",
        "    # dictionaty2 - file dictionary (id to file name)\n",
        "    dict_files = {}\n",
        "\n",
        "    # goes through each file in the list of file names\n",
        "    for i in file_names:\n",
        "        # if counter <= 150: #don't need this if statement, just using it so Jupyter doesn't crash from reading all the files\n",
        "\n",
        "            # gets the working directory (this may be different for everyone)\n",
        "            file_path = os.getcwd() + \"/data/\" + i\n",
        "            # opens the file in reading mode\n",
        "            file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
        "            # calls the function written for question 1 of the assignment\n",
        "            words = text_preprocessing(file)\n",
        "            #add the word and file name to the inverted index\n",
        "            for word in words:\n",
        "                inverted_index.insert(word, counter) #inserting counter instead of doc name (change later?)\n",
        "\n",
        "\n",
        "        # increment the counter\n",
        "            counter+=1\n",
        "\n",
        "        # closes the file\n",
        "            file.close()\n",
        "    return inverted_index\n",
        "\n",
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2ibdHAyEJ_qy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "1451093a-1aa7-4e82-ffad-214f01ecc573"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'list' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-07d9e8eb65ed>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minverted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_inverted_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(len(inverted_index))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-73eba136df9b>\u001b[0m in \u001b[0;36mcreate_inverted_index\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# calls the function written for question 1 of the assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m#add the word and file name to the inverted index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-e499a3bb35f4>\u001b[0m in \u001b[0;36mtext_preprocessing\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# remove duplicates - less words to do the next steps with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnewtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# c. remove any stop words from the NLTK package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ],
      "source": [
        "inverted_index = create_inverted_index()\n",
        "print(inverted_index.table)\n",
        "# print(len(inverted_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOKDGJKrqYUS"
      },
      "source": [
        "## Question 3: Query Support\n",
        "Support the following queries, where x and y would be taken as input from the user\n",
        "\n",
        "1. x OR y\n",
        "\n",
        "2. x AND y\n",
        "\n",
        "3. x AND NOT y\n",
        "\n",
        "4. x OR NOT y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4fxrPEx1qYUS"
      },
      "outputs": [],
      "source": [
        "def query(sentence, operation, InvertedIndex):\n",
        "  comparisons = 0\n",
        "  final_list = list() #list(dict.fromkeys(updated_list))\n",
        "\n",
        "  #OR\n",
        "  if \"OR\" in operation:\n",
        "    index = 0\n",
        "\n",
        "    #use a loop to find the index in the operation array where OR exists\n",
        "    for item in operation:\n",
        "      if item == \"OR\":\n",
        "        break\n",
        "      index += 1\n",
        "\n",
        "    #based on the index found, find the words in the sentence array that the operation is performed on\n",
        "    word1 = sentence[index]\n",
        "    word2 = sentence[index + 1]\n",
        "    print(\"OR\", word1)\n",
        "    print(\"OR\", word2)\n",
        "\n",
        "    #find the inverted indexes for the two words\n",
        "    list1 = InvertedIndex.retrieve(word1)\n",
        "    list2 = InvertedIndex.retrieve(word2)\n",
        "\n",
        "    #using the two inverted indexes to find the union of both (documents where either word exists)\n",
        "    list_docs = list1.union(list2)\n",
        "\n",
        "    #add total documents to comparisons variable\n",
        "    comparisons += len(list_docs)\n",
        "\n",
        "  #AND\n",
        "  if \"AND\" in operation:\n",
        "    index = 0\n",
        "\n",
        "  #use a loop to find the index in the operation array where AND exists\n",
        "    for item in operation:\n",
        "      if item == \"AND\":\n",
        "        break\n",
        "      index += 1\n",
        "\n",
        "  #based on the index found, find the words in the sentence array that the operation is performed on\n",
        "    word1 = sentence[index]\n",
        "    word2 = sentence[index + 1]\n",
        "\n",
        "  #find the inverted indexes for the two words\n",
        "    list1 = InvertedIndex.retrieve(word1)\n",
        "    list2 = InvertedIndex.retrieve(word2)\n",
        "\n",
        "  #using the two inverted indexes to find the intersection of both (documents where both words exists)\n",
        "    list_docs = list1.intersection(list2)\n",
        "\n",
        "  #add total documents to comparisons variable\n",
        "    comparisons += len(list_docs)\n",
        "\n",
        "  #AND NOT\n",
        "  #lion minus stood (lion - intersection)\n",
        "  if \"AND NOT\" in operation:\n",
        "    index = 0\n",
        "\n",
        "  #use a loop to find the index in the operation array where AND NOT exists\n",
        "    for item in operation:\n",
        "      if item == \"AND NOT\":\n",
        "        break\n",
        "      index += 1\n",
        "\n",
        "  #based on the index found, find the words in the sentence array that the operation is performed on\n",
        "    word1 = sentence[index]\n",
        "    word2 = sentence[index + 1]\n",
        "\n",
        "  #find the inverted indexes for the two words\n",
        "    list1 = InvertedIndex.retrieve(word1)\n",
        "    list2 = InvertedIndex.retrieve(word2)\n",
        "\n",
        "  #using the two inverted indexes to find where the first one exists but the second does not\n",
        "    list_docs = list1 - (list1.intersection(list2))\n",
        "\n",
        "  #add total documents to comparisons variable\n",
        "    comparisons += len(list_docs)\n",
        "\n",
        "  #OR NOT\n",
        "  if \"OR NOT\" in operation:\n",
        "    index = 0\n",
        "\n",
        "  #use a loop to find the index in the operation array where OR NOT exists\n",
        "    for item in operation:\n",
        "      if item == \"OR NOT\":\n",
        "        break\n",
        "      index += 1\n",
        "\n",
        "  #based on the index found, find the words in the sentence array that the operation is performed on\n",
        "    word1 = sentence[index]\n",
        "    word2 = sentence[index + 1]\n",
        "\n",
        "  #find the inverted indexes for the two words\n",
        "    list1 = InvertedIndex.retrieve(word1)\n",
        "    list2 = InvertedIndex.retrieve(word2)\n",
        "\n",
        "  #using the two inverted indexes to find where the first word exists or the second word does not\n",
        "    temp = list2.union(list1.intersection(list2))\n",
        "    list_docs = set(dict_files.values()) - list2\n",
        "\n",
        "    comparisons += len(list_docs)\n",
        "\n",
        "  return comparisons\n",
        "\n",
        "\n",
        "# a = [\"lion\", \"stood\"]\n",
        "# b = [\"OR NOT\"]\n",
        "# count = query(a, b, inverted_index)\n",
        "# print(count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing_str(query):\n",
        "\n",
        "    # a. converting to lowercase\n",
        "    lines = [x.lower() for x in query]\n",
        "\n",
        "    # join all the lines into one string\n",
        "    data = \"\".join(lines)\n",
        "\n",
        "    # b. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "\n",
        "    # remove duplicates - less words to do the next steps with\n",
        "    newtoken = list(dict.fromkeys(tokenization))\n",
        "\n",
        "    # c. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in newtoken:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "    # print(stop_words)\n",
        "\n",
        "    # d. Remove special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
        "    alphanumerictxt = []\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in stop_words:\n",
        "        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n",
        "\n",
        "    # e. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in alphanumerictxt:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "    # after all the other steps are completed check again to make sure no duplicated values\n",
        "    final_list = list(dict.fromkeys(updated_list))\n",
        "\n",
        "\n",
        "    return final_list\n"
      ],
      "metadata": {
        "id": "YOZ06gZqn4kI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = text_preprocessing_str(\"telephone, paved, roads\")\n",
        "op = [\"OR NOT\", \"AND NOT\"]\n",
        "# test2 = _query(\"telephone OR NOT paved AND NOT roads\", inverted_index)\n",
        "# test = text_preprocessing(\"telephone\")\n",
        "test2 = query(test, op, inverted_index)\n",
        "print(test2)"
      ],
      "metadata": {
        "id": "dZ3Dicb67iiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "2f099b24-94c8-47b3-ae64-b077bcc0acb0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'list' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9c372b2e5300>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preprocessing_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"telephone, paved, roads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"OR NOT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AND NOT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# test2 = _query(\"telephone OR NOT paved AND NOT roads\", inverted_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# test = text_preprocessing(\"telephone\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-b399332cc4ed>\u001b[0m in \u001b[0;36mtext_preprocessing_str\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# remove duplicates - less words to do the next steps with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnewtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# c. remove any stop words from the NLTK package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrJoDGwgqYUS"
      },
      "source": [
        "## Question 4: System Evaluation\n",
        "Evaluate your system against the set of provided queries.\n",
        "\n",
        "1. Query #1\n",
        "\n",
        "- Input Sentence: \"lion stood thoughtfully for a moment\"\n",
        "\n",
        "- Input operation sequence: [OR, OR, OR]\n",
        "\n",
        "- Expected preprocessed query: \"lion OR stood OR thoughtfully OR moment\"\n",
        "\n",
        "2. Query #2\n",
        "\n",
        "- Input Sentence: \"telephone, paved, roads\"\n",
        "\n",
        "- Input operation sequence: [OR NOT, AND NOT]\n",
        "\n",
        "- Expected preprocessed query: \"telephone OR NOT paved AND NOT roads\n",
        "\n",
        "\n",
        "Output should include:\n",
        "- The number of documents retrieved\n",
        "\n",
        "- The minimum number of total comparisons made (only for merging the algorithm)\n",
        "\n",
        "- The list of retrieved document names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOhdl5F_qYUT"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT3YvvnvqYUT"
      },
      "outputs": [],
      "source": [
        "# displaying the required print statements\n",
        "print(\"Number of matched documents: \")\n",
        "print(\"Minimum number of comparisons required\")\n",
        "print(\"List of retreived document names: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = {\"t\", \"u\"}\n",
        "b = {\"a\", \"u\"}\n",
        "a.union(b)"
      ],
      "metadata": {
        "id": "s-BxiPcJarsy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
