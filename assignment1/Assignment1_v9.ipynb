{"cells":[{"cell_type":"markdown","metadata":{"id":"vFj5zP--qYUN"},"source":["# CP423: Assignment 1\n","\n","Group Number: 5\n","\n","Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QOMJiPs5qYUP","executionInfo":{"status":"ok","timestamp":1716615715384,"user_tz":240,"elapsed":155,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["# MIGHT NEED TO RUN THIS FOR THE NLTK\n","#pip install certifi"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"FHuZV0qFqYUQ","executionInfo":{"status":"ok","timestamp":1716615716156,"user_tz":240,"elapsed":127,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["import certifi\n","import ssl\n","import os\n","\n","os.environ[\"SSL_CERT_FILE\"] = certifi.where()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUAdJUSpqYUQ","outputId":"28e0fd44-8894-4dcb-e71f-62636cf39b2b","executionInfo":{"status":"ok","timestamp":1716615720214,"user_tz":240,"elapsed":3201,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# inputs\n","import os\n","import re\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"fMI_dT14qYUQ"},"source":["## Question 1 : Text Preprocessing\n","a. Convert all text to lowercase\n","\n","b. Tokenize the text using NLTK\n","\n","c. Remove stop words using NLTK\n","\n","d. Exclude special characters except alphanumeric characters\n","\n","e. Eliminate singly occurring characters\n","\n","f. Create a set of all the words"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Kd0diRyRvJHh","executionInfo":{"status":"ok","timestamp":1716615720347,"user_tz":240,"elapsed":3,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["def text_preprocessing_str(query):\n","\n","    # a. converting to lowercase\n","    lines = [x.lower() for x in query]\n","\n","    # join all the lines into one string\n","    data = \"\".join(lines)\n","\n","    # b. using the tokenization package to tokenize the words\n","    tokenization = word_tokenize(data)\n","\n","    # remove duplicates - less words to do the next steps with\n","    newtoken = list(dict.fromkeys(tokenization))\n","\n","    # c. remove any stop words from the NLTK package\n","    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n","\n","    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n","    stop_words = []\n","    for word in newtoken:\n","            if word not in stop_words_lib:\n","                 stop_words.append(word)\n","\n","    # print(stop_words)\n","\n","    # d. Remove special characters\n","    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n","    alphanumerictxt = []\n","    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n","    # strips the empty string afterwards\n","    for i in stop_words:\n","        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n","\n","    # e. eliminating any characters of length 1\n","    updated_list = []\n","\n","    # goes through each index in the list and if the length is more than 1 will append to new list\n","    for i in alphanumerictxt:\n","        if len(i)>1:\n","            updated_list.append(i)\n","\n","    # after all the other steps are completed check again to make sure no duplicated values\n","    final_list = list(dict.fromkeys(updated_list))\n","\n","\n","    return final_list"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rigjoVIdqYUR","executionInfo":{"status":"ok","timestamp":1716615723503,"user_tz":240,"elapsed":120,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["def text_preprocessing(file):\n","\n","    # reading the file line by line\n","    lines = file.readlines()\n","\n","    # a. converting to lowercase\n","    lines = [x.lower() for x in lines]\n","\n","    # join all the lines into one string\n","    data = \"\".join(lines)\n","\n","    final_list = text_preprocessing_str(data)\n","\n","    #f\n","    return final_list\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-9skAm3HqYUR","executionInfo":{"status":"ok","timestamp":1716615725709,"user_tz":240,"elapsed":106,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["# import data files\n","def readfile_fun():\n","  # takes the names of the files in the folder and creates a list\n","  file_names = os.listdir(\"./data\")\n","  # file_names\n","\n","  # file id (key)\n","  counter = 1\n","\n","  #dictionary1 - translates the ID which belong to the file\n","  dict_words = {}\n","  # dictionaty2 - file dictionary (id to file name)\n","  dict_files = {}\n","\n","  # goes through each file in the list of file names\n","  for i in file_names:\n","\n","      # if counter <= 2: #for checking purposes - can remove before submitting\n","\n","        # gets the working directory (this may be different for everyone)\n","        file_path = os.getcwd() + \"/data/\" + i\n","        # opens the file in reading mode\n","        file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n","        # calls the function written for question 1 of the assignment\n","        words = text_preprocessing(file)\n","\n","        # updates the dictionaries\n","        dict_words[counter] = words # list of words in this file\n","        dict_files[counter] = i # list of all the file names\n","\n","      # increment the counter\n","        counter+=1\n","\n","      # closes the file\n","        file.close()\n","\n","  print(dict_files)\n","  print(dict_words)\n","\n","  return dict_words, dict_files"]},{"cell_type":"markdown","metadata":{"id":"auQGFmd1qYUR"},"source":["## Question 2 : Inverted Index Implementation\n","Implement an inverted index data structure for the preprocessed dataset\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"8fvqXm-KJ_qy","executionInfo":{"status":"ok","timestamp":1716615727875,"user_tz":240,"elapsed":132,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["class HashInvertedIndex:\n","\n","    # initialize hash table\n","    def __init__(self, capacity=100): # manually set the capacity\n","        self.capacity = capacity\n","        self.table = [[] for _ in range(capacity)]\n","        self.count = 0\n","\n","    # finding the corresponding value for the key\n","    def _index(self, word):\n","        index = hash(word) % self.capacity\n","        return index\n","\n","    # add key-value pair to the hash table\n","    def insert(self, key, value):\n","\n","        # compute the hash index for the word\n","        hash_index = self._index(key)\n","        for i, kv in enumerate(self.table[hash_index]):\n","            k, v = kv\n","            if k == key:\n","                # add the value to the existing set of values\n","                v.add(value)\n","                # add the entry to the table\n","                self.table[hash_index][i] = (k,v)\n","                return\n","        self.table[hash_index].append((key, {value}))\n","\n","    # retrieve the set of values for a given key, if not found, returns an empty set\n","    def retrieve(self, key):\n","        # compute the hash index for a given key\n","        hash_index = self._index(key)\n","        for k, v in self.table[hash_index]:\n","            # if the key if found, return the set\n","            if k == key:\n","                return v\n","        return set()\n","\n","    # for testing purposes - shows us the string representation of the hash tables\n","    def __rep__(self):\n","        return str(self.table)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jV-QPzJvJ_qy","executionInfo":{"status":"ok","timestamp":1716615729878,"user_tz":240,"elapsed":106,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["# build the inverted index - reads the documents and their corresponding words\n","def create_inverted_index():\n","\n","    # initialize the hash table/inverted index\n","    inverted_index = HashInvertedIndex()\n","\n","    # calls the function above\n","    # returns list of words in doc and list of docs\n","    words, files = readfile_fun()\n","\n","    # fill in the inverted index\n","    for key, value in words.items():\n","        for word in value:\n","            inverted_index.insert(word, key) # inserting counter instead of doc name\n","\n","    return inverted_index, files, words\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1Ov9lcxJbMdgwst1KqopThy628Ha0CXFr"},"id":"2ibdHAyEJ_qy","outputId":"9d01ec20-55d0-4c0c-da2c-3433d1f2430a","executionInfo":{"status":"ok","timestamp":1716617162063,"user_tz":240,"elapsed":26241,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# calls the function\n","inverted_index, file_list, word_list = create_inverted_index()\n","\n","# prints out the index and the list of file names\n","# index to first list is hash to the word\n","for i in range(len(inverted_index.table)):\n","    #for j in i:\n","    # prints the hash buckets\n","    print(inverted_index.table[i])\n","    # prints the length of each bucket (right now around 400-500 - evenly distributed) - if we want smaller amounts in each bucket set bucket_size = 200\n","    #print(len(inverted_index.table[i]))\n","\n","#print(file_list)\n"]},{"cell_type":"markdown","metadata":{"id":"dOKDGJKrqYUS"},"source":["## Question 3: Query Support\n","Support the following queries, where x and y would be taken as input from the user\n","\n","1. x OR y\n","\n","2. x AND y\n","\n","3. x AND NOT y\n","\n","4. x OR NOT y\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"LkdaChngvJHj","executionInfo":{"status":"ok","timestamp":1716617852530,"user_tz":240,"elapsed":121,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[],"source":["def query(sentence, operation, InvertedIndex, file):\n","\n","  #set count of comparisons to 0 at beginning\n","  #set index to 0\n","  #set word to the firts word in the sentence\n","  #set final_list to the inverted index for the first word in the sentence\n","\n","  comparisons = 0\n","  index = 0\n","  word = sentence[index]\n","  final_list = InvertedIndex.retrieve(word)\n","\n","  #loop through array of operations\n","  while index < len(operation):\n","\n","    #OR\n","    if operation[index] == \"OR\":\n","\n","      # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the class in Q2\n","      next_word = sentence[index+1]\n","      list1 = InvertedIndex.retrieve(next_word)\n","\n","      # looking at the minimum of the smallest list as UNION will put every value from the min list into the max list removing duplicates\n","      # using the final_list as the base, counting comparisons made (length of smaller list)\n","      comparisons += min(len(final_list),len(list1))\n","\n","      # using the union function to combine the final_list with list1 in order to capture all documents with both words\n","      final_list = final_list.union(list1)\n","      # print(final_list)\n","\n","    #AND\n","    elif operation[index] == \"AND\":\n","\n","      # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the class in Q2\n","      next_word = sentence[index+1]\n","      list1 = InvertedIndex.retrieve(next_word)\n","\n","      # looking at the minimum of the smallest list as INTERSECTION will keep matching values in the max list removing duplicates\n","      # using the final_list as the base, counting comparisons made (length of smaller list)\n","      comparisons += min(len(final_list),len(list1))\n","\n","      final_list = final_list.intersection(list1)\n","\n","    elif operation[index] == \"AND NOT\":\n","\n","      # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the class in Q2\n","      next_word = sentence[index+1]\n","      list1 = InvertedIndex.retrieve(next_word)\n","\n","      # to get the intersection\n","\n","      # looking at the minimum of the smallest list, keeping count of the values compared to the base list\n","      # adding in the values in the intersection of both lists, as values are compared for items that exist in both\n","\n","      comparisons += min(len(final_list),len(list1))\n","      comparisons += len(final_list.intersection(list1))\n","\n","      # removing values that exist in both lists from the list of values for the first word (values that exist for word1 and not word2)\n","      final_list = final_list - (final_list.intersection(list1))\n","\n","    elif operation[index] == \"OR NOT\":\n","\n","      # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the class in Q2\n","      next_word = sentence[index+1]\n","      list1 = InvertedIndex.retrieve(next_word)\n","\n","      # using the two inverted indexes to find where the first word exists or the second word does not\n","      # creating temp variable to store list of values where only second word is present\n","      temp = list1 - (list1.intersection(final_list))\n","\n","      # looking at the minimum of the smallest list, keeping count of the values compared to the base list\n","      # adding in the values in the intersection of both lists, as values are compared for items that exist in both\n","      # adding in the values for comparisons done where only the second word is present\n","\n","      comparisons += min(len(final_list),len(list1))\n","      comparisons += len(final_list.intersection(list1))\n","      comparisons += len(temp)\n","\n","      # using all document names/values as a universal set, and removing document names/values where only the second word exists\n","      # final_list = set(file.values()) - temp\n","      final_list = set(file.keys()) - temp\n","      print(final_list)\n","\n","    index+=1\n","\n","  return final_list, comparisons\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CrJoDGwgqYUS"},"source":["## Question 4: System Evaluation\n","Evaluate your system against the set of provided queries.\n","\n","1. Query #1\n","\n","- Input Sentence: \"lion stood thoughtfully for a moment\"\n","\n","- Input operation sequence: [OR, OR, OR]\n","\n","- Expected preprocessed query: \"lion OR stood OR thoughtfully OR moment\"\n","\n","2. Query #2\n","\n","- Input Sentence: \"telephone, paved, roads\"\n","\n","- Input operation sequence: [OR NOT, AND NOT]\n","\n","- Expected preprocessed query: \"telephone OR NOT paved AND NOT roads\n","\n","\n","Output should include:\n","- The number of documents retrieved\n","\n","- The minimum number of total comparisons made (only for merging the algorithm)\n","\n","- The list of retrieved document names"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"JOhdl5F_qYUT","outputId":"c04f1908-c853-4ed5-c153-61e3bf173560","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716617859324,"user_tz":240,"elapsed":4215,"user":{"displayName":"Myisha Chaudhry","userId":"05233120116377599077"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter your string: hello world\n","Enter the operations (in CAPS, seperated with commas, NO SPACES between the operations): OR NOT\n","Sentence:  ['hello', 'world']\n","Operations list:  ['OR NOT']\n","{2, 3, 5, 10, 11, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 33, 37, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 53, 54, 55, 57, 58, 60, 61, 63, 64, 65, 67, 68, 70, 72, 73, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 87, 89, 90, 91, 93, 94, 95, 97, 98, 101, 103, 104, 105, 107, 108, 110, 112, 114, 115, 117, 118, 121, 122, 123, 124, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 143, 146, 148, 150, 152, 155, 157, 158, 159, 160, 163, 166, 167, 168, 169, 170, 171, 172, 174, 177, 178, 180, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 204, 206, 208, 209, 210, 211, 213, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 228, 229, 230, 231, 232, 233, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249}\n","Number of Matched documents:  174\n","Minimum number of comparisons required:  127\n","List of retreived document names: \n","blh.txt\n","frogp.txt\n","campfire.txt\n","tctac.txt\n","bgcspoof.txt\n","foxncrow.txt\n","vampword.txt\n","goldfish.txt\n","snowmaid.txt\n","vaincrow.txt\n","taxnovel.txt\n","bulzork1.txt\n","wolflamb.txt\n","vainsong.txt\n","hitch3.txt\n","dtruck.txt\n","hansgrtl.txt\n","hotline1.txt\n","gemdra.txt\n","psf.txt\n","weaver.txt\n","goldgoos.txt\n","bishop00.txt\n","sunday.txt\n","space.txt\n","bureau.txt\n","cybersla.txt\n","monkking.txt\n","melissa.txt\n","bulhuntr.txt\n","ab40thv.txt\n","lpeargrl.txt\n","flute.txt\n","batlslau.txt\n","aislesix.txt\n","fable.txt\n","elveshoe.txt\n","dopedenn.txt\n","mtinder.txt\n","berternie.txt\n","musgrave.txt\n","bulmrx.txt\n","beautbst.txt\n","pregn.txt\n","bulphrek.txt\n","modemhippy.txt\n","angry_ca.txt\n","kneeslapper.txt\n","kzap.txt\n","uglyduck.txt\n","cabin.txt\n","bulolli1.txt\n","lure.txt\n","gloves.txt\n","lament.txt\n","shrdfarm.txt\n","startrek.txt\n","tcoa.txt\n","lionwar.txt\n","buldream.txt\n","lmtchgrl.txt\n","clevdonk.txt\n","hell4.txt\n","life.txt\n","aminegg.txt\n","bullove.txt\n","encamp01.txt\n","alissadl.txt\n","bulprint.txt\n","aircon.txt\n","cardcnt.txt\n","the-tree.txt\n","horswolf.txt\n","girlclub.txt\n","foxnstrk.txt\n","lrrhood.txt\n","socialvikings.txt\n","pussboot.txt\n","parotsha.txt\n","sre-dark.txt\n","mike.txt\n","pepdegener.txt\n","glimpse1.txt\n","hitch2.txt\n","4moons.txt\n","hareporc.txt\n","foxngrap.txt\n","tree.txt\n","mydream.txt\n","adler.txt\n","mouslion.txt\n","bagelman.txt\n","plescopm.txt\n","long1-3.txt\n","fish.txt\n","roger1.txt\n","tearglas.txt\n","snow.txt\n","poem-1.txt\n","fleas.txt\n","graymare.txt\n","fred.txt\n","pphamlin.txt\n","abyss.txt\n","stairdre.txt\n","sick-kid.txt\n","sight.txt\n","wolfcran.txt\n","emperor3.txt\n","healer.txt\n","monksol.txt\n","hotline3.txt\n","bulnoopt.txt\n","tailbear.txt\n","hellmach.txt\n","floobs.txt\n","blackp.txt\n","poplstrm.txt\n","horsdonk.txt\n","greedog.txt\n","oxfrog.txt\n","enginer.txt\n","lionmosq.txt\n","game.txt\n","crabhern.txt\n","dicksong.txt\n","sucker.txt\n","snowqn1.txt\n","cmoutmou.txt\n","spiders.txt\n","retrib.txt\n","buldetal.txt\n","flktrp.txt\n","5orange.txt\n","poem-4.txt\n","bulolli2.txt\n","traitor.txt\n","aesopa10.txt\n","discocanbefun.txt\n","weeprncs.txt\n","adv_alad.txt\n","13chil.txt\n","7oldsamr.txt\n","timetrav.txt\n","dakota.txt\n","hareleph.txt\n","rid.txt\n","sanpedr2.txt\n","unluckwr.txt\n","kharian.txt\n","hole2nar.txt\n","telefone.txt\n","bgb.txt\n","bluebrd.txt\n","helmfuse.txt\n","blind.txt\n","3wishes.txt\n","gold3ber.txt\n","bulnland.txt\n","buggy.txt\n","100west.txt\n","vgilante.txt\n","flytrunk.txt\n","tinsoldr.txt\n","running.txt\n","musibrem.txt\n","antcrick.txt\n","cooldark.txt\n","jackbstl.txt\n","pinocch.txt\n","sleprncs.txt\n","6ablemen.txt\n","obstgoat.txt\n","arctic.txt\n"]}],"source":["text = input(\"Enter your string: \")\n","sentence_list = text_preprocessing_str(text)\n","\n","operations = input(\"Enter the operations (in CAPS, seperated with commas, NO SPACES between the operations): \")\n","operations_list = operations.split(\",\")\n","print(\"Sentence: \" ,sentence_list)\n","print(\"Operations list: \", operations_list)\n","match_list, comp = query(sentence_list, operations_list, inverted_index,file_list)\n","\n","\n","print(\"Number of Matched documents: \" , len(match_list))\n","print(\"Minimum number of comparisons required: \", comp)\n","print(\"List of retreived document names: \")\n","for match in match_list:\n","     print(file_list[match])"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}