{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxLQqcGUMZzy"
      },
      "source": [
        "# CP423: Assignment 1\n",
        "\n",
        "Group Number: 5\n",
        "\n",
        "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sa7a7Y2MZz3"
      },
      "outputs": [],
      "source": [
        "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
        "#pip install certifi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9LS0fmZMZz4"
      },
      "outputs": [],
      "source": [
        "import certifi\n",
        "import ssl\n",
        "import os\n",
        "\n",
        "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6qXrKWLMZz5",
        "outputId": "e631ead3-65b9-443d-f371-e75f4cecd886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# inputs\n",
        "import os\n",
        "import re\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF88-IvYMZz6"
      },
      "source": [
        "### Question 1: Positional Index\n",
        "\n",
        "a. Execute the subsequent preprocessing tasks on the provided dataset:\n",
        "\n",
        "1. Transform the text to lowercase\n",
        "\n",
        "2. Perform word tokenization\n",
        "\n",
        "3. Eliminate stopwords from the tokens\n",
        "\n",
        "4. Remove punctuation marks from the tokens\n",
        "\n",
        "5. Eliminate empty space tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjAiy0ziMZz6"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing_file(file):\n",
        "\n",
        "    # reading the file line by line\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # 1. converting to lowercase\n",
        "    lines = [x.lower() for x in lines] # list\n",
        "\n",
        "    # data = data.replace(\"-\", \" \")\n",
        "    # data = data.replace(\"'\", \" \")\n",
        "    # doing this here because the positional dictionary gets all the punctuation which then does not work\n",
        "    # 4. Remove punctuation / special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
        "    alphanumerictxt = []\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in lines:\n",
        "        stripped = re.sub(pattern,\" \", i).strip()\n",
        "        split_stripped = stripped.split() # list\n",
        "        for j in split_stripped:\n",
        "            alphanumerictxt.append(j)\n",
        "\n",
        "     # join all the lines into one string\n",
        "    data = \" \".join(alphanumerictxt)\n",
        "\n",
        "    # 2. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "    # print(tokenization)\n",
        "\n",
        "    # 3. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in tokenization:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "    # 5. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in stop_words:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "    # store the dictionary of positions / location for each word\n",
        "    dict_position = {} # dictionary has a word and the list of the position\n",
        "    position_counter = 1\n",
        "    # traverse through the list of all words\n",
        "    for word in updated_list:\n",
        "        # everytime it finds a new word it will create a new key and create a list for the position\n",
        "        if word not in dict_position:\n",
        "            dict_position[word] = [position_counter]\n",
        "        else: # the word already exists so just get the position it is found in\n",
        "\n",
        "             dict_position[word].append(position_counter)\n",
        "        position_counter+=1\n",
        "\n",
        "    #print(dict_position[\"by\"])\n",
        "\n",
        "\n",
        "    # gets the positional index for the final list\n",
        "    dict_final = {}\n",
        "    for element in dict_position:\n",
        "        # print(\"Element: \", element)\n",
        "        dict_final[element] = dict_position[element]\n",
        "\n",
        "    return dict_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFhx1NnvMZz7"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing_query(query):\n",
        "\n",
        "    # a. converting to lowercase\n",
        "    lines = [x.lower() for x in query]\n",
        "\n",
        "    # join all the lines into one string\n",
        "    data = \"\".join(lines)\n",
        "\n",
        "    # b. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "\n",
        "    # remove duplicates - less words to do the next steps with\n",
        "    newtoken = list(dict.fromkeys(tokenization))\n",
        "\n",
        "    # c. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in newtoken:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "    # print(stop_words)\n",
        "\n",
        "    # d. Remove special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
        "    alphanumerictxt = []\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in stop_words:\n",
        "        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n",
        "\n",
        "    # e. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in alphanumerictxt:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "    # after all the other steps are completed check again to make sure no duplicated values\n",
        "    final_list = list(dict.fromkeys(updated_list))\n",
        "\n",
        "\n",
        "    return final_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh4kjbGrMZz8"
      },
      "outputs": [],
      "source": [
        "# import data files\n",
        "def readfile_fun():\n",
        "  # takes the names of the files in the folder and creates a list\n",
        "  file_names = os.listdir(\"./data\")\n",
        "\n",
        "  # file id (key)\n",
        "  counter = 1\n",
        "\n",
        "  #dictionary1 - gets the list of words, frequency and position\n",
        "  # i.e : to : 4 [1,3,5,7]\n",
        "  dict_words = {}\n",
        "  #dictionary2 - file dictionary (id to file name)\n",
        "  dict_files = {}\n",
        "\n",
        "  # goes through each file in the list of file names\n",
        "  for i in file_names:\n",
        "\n",
        "    if counter <= 3: #for checking purposes - can remove before submitting\n",
        "        # gets the working directory (this may be different for everyone)\n",
        "        file_path = os.getcwd() + \"/data/\" + i\n",
        "        # opens the file in reading mode\n",
        "        file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
        "        # calls the function\n",
        "        # words is a dictionary\n",
        "        words = text_preprocessing_file(file)\n",
        "\n",
        "        # updates the dictionaries\n",
        "        dict_words[counter] = words # dictionary of dictionary\n",
        "        dict_files[counter] = i # list of all the file names\n",
        "\n",
        "        # increment the counter\n",
        "        counter+=1\n",
        "\n",
        "        # closes the file\n",
        "        file.close()\n",
        "\n",
        "  return dict_words, dict_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3F66hW_MZz8"
      },
      "source": [
        "### Question 1 - Positional Index\n",
        "b. Develop the positional index data structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1gg8XSZMZz8"
      },
      "outputs": [],
      "source": [
        "#b.\n",
        "class Node:\n",
        "    # value: docID, count and list of position\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.next = None\n",
        "\n",
        "# linked list for the document id\n",
        "class LinkedList:\n",
        "\n",
        "    # initialize linked list\n",
        "    def __init__(self):\n",
        "        self._front = None\n",
        "        self._rear = None\n",
        "        self._counter = 0\n",
        "\n",
        "    # key is the word value is the docID\n",
        "    # insert to theh beginning\n",
        "    def insert_front(self, value):\n",
        "        # if it is an empty list\n",
        "        newnode = Node(value)\n",
        "        if self._front is None:\n",
        "            self._front = newnode\n",
        "            self._rear = newnode\n",
        "        else:\n",
        "            newnode.next = self._front\n",
        "            self._front = newnode\n",
        "        self._counter +=1\n",
        "        return\n",
        "\n",
        "    def insert_rear(self, value):\n",
        "        # if it is an empty list\n",
        "        newnode = Node(value)\n",
        "        if self._front is None:\n",
        "            self._front = newnode\n",
        "            self._rear = newnode\n",
        "        else:\n",
        "            self._rear.next = newnode\n",
        "            self._rear = newnode\n",
        "        self._counter +=1\n",
        "        return\n",
        "\n",
        "    # for testing purposes - shows us the string representation of the linked list\n",
        "    def printList(self):\n",
        "        temp = self._front\n",
        "        while temp is not None:\n",
        "            print(temp.value)\n",
        "            temp = temp.next\n",
        "        return\n",
        "\n",
        "    def merge(self):\n",
        "        merged_list = []\n",
        "        temp = self._front\n",
        "        while temp is not None:\n",
        "            merged_list.append(temp.value)\n",
        "            temp = temp.next\n",
        "        return merged_list\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        temp = self._front\n",
        "        newstring = \"\"\n",
        "        while temp is not None:\n",
        "            newstring += \" DocID: \"+ str(temp.value[0])\n",
        "            newstring += \", Frequency: \"+ str(temp.value[1])\n",
        "            newstring += \", Position: \"+ str(temp.value[2])+ \"\\n\"\n",
        "            temp = temp.next\n",
        "        return newstring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuGUeB3CMZz9"
      },
      "outputs": [],
      "source": [
        "# position index - dictionary\n",
        "class PositionIndex:\n",
        "    def __init__(self):\n",
        "        self.dict_post = {} # key = word, value = linked list\n",
        "\n",
        "    # key = word\n",
        "    def insert(self, key, docID, position):\n",
        "        newlist = [docID, len(position), position]\n",
        "        if key not in self.dict_post:\n",
        "            self.dict_post[key] = LinkedList() # created a new linked list for a new key\n",
        "        self.dict_post[key].insert_rear(newlist) # adds to the existing list\n",
        "        return\n",
        "\n",
        "    # takes one word and returns all the documents and positions\n",
        "    def retrieve(self, key): # take the words and returns the entire linked list --> if key is found then returns entire linked list\n",
        "        ret = []\n",
        "        if key in self.dict_post:\n",
        "            ret = self.dict_post[key].merge()\n",
        "        return ret\n",
        "\n",
        "    def retrieve_doc(self, key, docID):\n",
        "        #takes a list of docIDs and checks if that key is there - goes to the linked list and returns all the linked list of only the doc ids that match (sub linked list in that word)\n",
        "        doc_list = []\n",
        "        if key in self.dict_post:\n",
        "            word_list = self.dict_post[key].merge() # has the doc ids and positions\n",
        "            for doc in word_list:\n",
        "                if doc[0] == docID: # if the document id is the same\n",
        "                    doc_list.append(doc) # get the list\n",
        "        return doc_list\n",
        "\n",
        "    # stringify function\n",
        "    def __str__(self) -> str:\n",
        "        newstring = \"\"\n",
        "        for key in self.dict_post.keys():\n",
        "            newstring+= \"{ \" + key\n",
        "            newstring+= \": (\"+ str(self.dict_post[key]._counter) + \" {\\n\"\n",
        "            newstring+= str(self.dict_post[key])\n",
        "            newstring+= \"} ) } \\n\\n\"\n",
        "\n",
        "        return newstring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZMJIB8FMZz9"
      },
      "outputs": [],
      "source": [
        "class HashInvertedIndex:\n",
        "\n",
        "    # initialize hash table\n",
        "    def __init__(self, capacity=100): # manually set the capacity\n",
        "        self.capacity = capacity\n",
        "        self.table = [[] for _ in range(capacity)]\n",
        "        self.count = 0\n",
        "\n",
        "    # finding the corresponding value for the key\n",
        "    def _index(self, word):\n",
        "        index = hash(word) % self.capacity\n",
        "        return index\n",
        "\n",
        "    # add key-value pair to the hash table\n",
        "    def insert(self, key, value):\n",
        "\n",
        "        # compute the hash index for the word\n",
        "        hash_index = self._index(key)\n",
        "        for i, kv in enumerate(self.table[hash_index]):\n",
        "            k, v = kv\n",
        "            if k == key:\n",
        "                # add the value to the existing set of values\n",
        "                v.add(value)\n",
        "                # add the entry to the table\n",
        "                self.table[hash_index][i] = (k,v)\n",
        "                return\n",
        "        self.table[hash_index].append((key, {value}))\n",
        "\n",
        "    # retrieve the set of values for a given key, if not found, returns an empty set\n",
        "    def retrieve(self, key):\n",
        "        # compute the hash index for a given key\n",
        "        hash_index = self._index(key)\n",
        "        for k, v in self.table[hash_index]:\n",
        "            # if the key if found, return the set\n",
        "            if k == key:\n",
        "                return v\n",
        "        return set()\n",
        "\n",
        "    # for testing purposes - shows us the string representation of the hash tables\n",
        "    def __rep__(self):\n",
        "        return str(self.table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3gjxQvgMZz-"
      },
      "outputs": [],
      "source": [
        "# build the positional index - reads the documents and their corresponding words\n",
        "def create_positional_inverted_index():\n",
        "\n",
        "    # initialize the  linked list /inverted index\n",
        "    positional_index = PositionIndex()\n",
        "    # inverted_index = HashInvertedIndex()\n",
        "\n",
        "    # calls the function above\n",
        "    # returns list of words in doc and list of docs\n",
        "    words, files = readfile_fun()\n",
        "\n",
        "    # fill in the inverted index\n",
        "    # key = file IDs\n",
        "    # value = dictionary of words\n",
        "    for fileID, value in words.items():\n",
        "        for word in value.keys():\n",
        "            positional_index.insert(word, fileID, value[word]) # inserting counter instead of doc name\n",
        "\n",
        "    return positional_index, files, words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xFakfb9MZz-"
      },
      "outputs": [],
      "source": [
        "# build the inverted index - reads the documents and their corresponding words\n",
        "def create_inverted_index():\n",
        "\n",
        "    # initialize the  linked list /inverted index\n",
        "    inverted_index = HashInvertedIndex()\n",
        "\n",
        "    # calls the function above\n",
        "    # returns list of words in doc and list of docs\n",
        "    words, files = readfile_fun()\n",
        "    # fill in the inverted index\n",
        "    for key, value in words.items():\n",
        "        for word in value.keys():\n",
        "            inverted_index.insert(word, key) # inserting counter instead of doc name\n",
        "\n",
        "    return inverted_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNcUNcOTMZz-"
      },
      "source": [
        "### Question 1 - Positional Index\n",
        "\n",
        "c. Facilitate the searching of phrase queries, assuming the query length is equal to or less than 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y8JMfF4MZz_"
      },
      "outputs": [],
      "source": [
        "# C\n",
        "# takes query thats correct and give answer\n",
        "def find_common_docs(query, InvertedIndex):\n",
        "  index = 0\n",
        "  # print(query)\n",
        "  word = query[index]\n",
        "  doc_list = InvertedIndex.retrieve(word)\n",
        "  # FIND THE SHORTEST LIST\n",
        "  #loop through array of operations\n",
        "  while (index < len(query)-1):\n",
        "  # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the class in Q2\n",
        "    next_word = query[index+1]\n",
        "    list1 = InvertedIndex.retrieve(next_word)\n",
        "    doc_list = doc_list.intersection(list1)\n",
        "    index+=1\n",
        "\n",
        "  return doc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6ElqB_RMZz_"
      },
      "outputs": [],
      "source": [
        "#passing in a list which includes all the words in the query, docIDs, frequency and position\n",
        "# position_words = dictionary, dictionary, linked list\n",
        "def find_positions(query, pos_ind, common_doc):\n",
        "    # create new_dictionary to store the words and the position dictionary\n",
        "    new_dict={}\n",
        "    new_list = [] # list of all the positions\n",
        "    adj_list = [] # positions are adjusted for the query\n",
        "    count = len(query)\n",
        "    i = 0\n",
        "\n",
        "    # get the common documents and positions\n",
        "    for word in query:\n",
        "        new_dict[word] = pos_ind.retrieve_doc(word, common_doc)\n",
        "        # words with only common document with the positions\n",
        "    # print(new_dict)\n",
        "    for word in query:\n",
        "        new_list.append(new_dict[word][0][2]) # pull out the positional list (at index 2)\n",
        "        # print(word)\n",
        "    # print(new_list)\n",
        "\n",
        "    # shifts the positions of the words to get them the same\n",
        "    for i in range (count):\n",
        "        another_list= new_list[i] # for the first word get the positions\n",
        "        temp_list = [x + count - 1 - i for x in another_list] # for every element in the list going to add the specific value and put into a new list\n",
        "        adj_list.append(temp_list)\n",
        "    # print(adj_list)\n",
        "    # gets the intersection\n",
        "    i = 0\n",
        "    intersected_list = adj_list[i]\n",
        "    i+=1\n",
        "    for i in range (count):\n",
        "        next_list = adj_list[i]\n",
        "        intersected_list = set(intersected_list).intersection(next_list)\n",
        "    # print(intersected_list)\n",
        "\n",
        "    #unshift the positions\n",
        "    final_list = [x - (count - 1) for x in intersected_list]\n",
        "\n",
        "    return final_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvqeFfhjMZz_"
      },
      "source": [
        "### Question 1 - Positional Index\n",
        "\n",
        "Calling everything from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpqjR0gQMZz_"
      },
      "outputs": [],
      "source": [
        "pos_ind,file_list,position_words = create_positional_inverted_index()\n",
        "inverted_index = create_inverted_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEZ_4xVBMZ0A",
        "outputId": "d609e314-31c5-4241-cffc-7b164b20db64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'spider.txt', 2: 'tctac.txt', 3: 'glimpse1.txt'}\n"
          ]
        }
      ],
      "source": [
        "print(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJi7lzAMMZ0A",
        "outputId": "e73ed198-b59c-4689-fd5e-f7e549d21563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (up to 5 words) : hello\n",
            "Sentence 1 entered (cleaned):  ['hello']\n",
            "Number of matching documents:  0\n",
            "File name(s):  []\n"
          ]
        }
      ],
      "source": [
        "sentence = input(\"Enter your query (up to 5 words) : \" )\n",
        "# get every element of the word\n",
        "#stripped_sentence = sentence.split(\" \")\n",
        "# print(\"Sentence 1 entered: \", stripped_sentence)\n",
        "cleaned_sentence = text_preprocessing_query(sentence)\n",
        "print(\"Sentence 1 entered (cleaned): \" , cleaned_sentence)\n",
        "index = 1\n",
        "while len(cleaned_sentence) > 5:\n",
        "    index+= 1\n",
        "    sentence = input(\"Enter your query (up to 5 words) : \" )\n",
        "    cleaned_sentence = text_preprocessing_query(sentence)\n",
        "    print(\"Sentence%2d entered (cleaned): \"% index, cleaned_sentence)\n",
        "\n",
        "\n",
        "file_names = []\n",
        "if len(cleaned_sentence) > 0:\n",
        "    # finds all common documents\n",
        "    common_documents = find_common_docs(cleaned_sentence, inverted_index)\n",
        "    common_documents = list(common_documents)\n",
        "    count = 0\n",
        "    for doc in common_documents:\n",
        "        final = find_positions(cleaned_sentence, pos_ind, doc) # positions\n",
        "        if len(final)!= 0:\n",
        "            file_names.append(file_list[doc])\n",
        "            print(\"Document name: \",file_list[doc])\n",
        "            print(\"The query start positions: \", final)\n",
        "            count+=1\n",
        "    print(\"Number of matching documents: \", count)\n",
        "    print(\"File name(s): \",file_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMBWCkNLMZ0A"
      },
      "source": [
        "### Question 2: Ranking and Term-Weighting\n",
        "\n",
        "1. First, apply the preprocessing steps mentioned in Q1 to the given dataset:\n",
        "- Convert the text to lowercase.\n",
        "- Perform word tokenization.\n",
        "- Remove stopwords from the tokens.\n",
        "- Remove punctuation marks from the tokens.\n",
        "- Remove blank space tokens.\n",
        "2. Construct the matrix with dimensions (number of documents) x (vocabulary size).\n",
        "3. Populate the TF-IDF values in the matrix for each word in the vocabulary.\n",
        "4. Create a query vector with a size equal to the vocabulary.\n",
        "5. Calculate the TF-IDF score for the query using the TF-IDF matrix. Identify the top\n",
        "5 relevant documents based on the score.\n",
        "6. Apply all 5 weighting schemes for term frequency calculation and provide the TF-\n",
        "IDF score and results for each scheme individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwIajN83MZ0B"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "#HAVE TO LIMIT ROWS AND COLS, WILL NOT RUN ON GOOGLE COLLAB\n",
        "# creates an empty TF-IDF matrix\n",
        "\n",
        "def tf_idf_matrix(doc_num, pos_ind):\n",
        "  rows = doc_num\n",
        "  cols = len(pos_ind.dict_post)\n",
        "  matrix = np.empty((rows, cols), dtype = object).reshape(rows, cols)\n",
        "\n",
        "  return matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uTUatTyMZ0B"
      },
      "outputs": [],
      "source": [
        "# Weighting Schemes\n",
        "def weighting(pos_ind, docId, word, weight):\n",
        "  tf = 0\n",
        "\n",
        "  # 1) Binary, if word exists in documents returns 1, else 0\n",
        "  if weight == 1:\n",
        "    found = pos_ind.retrieve(word) # finds list of documents that has word\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "    else:\n",
        "      tf = 1\n",
        "\n",
        "  # 2) Raw Count, getting the term frequency for the given word in the given document\n",
        "  elif weight == 2:\n",
        "    raw = pos_ind.retrieve(word)\n",
        "    for item in raw:\n",
        "      if item[0] == docId:\n",
        "        tf = item[1]\n",
        "        break;\n",
        "\n",
        "  # 3) Term Frequency, dividing raw count by compiment of chosen term\n",
        "  elif weight == 3:\n",
        "    found = pos_ind.retrieve(word)\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "\n",
        "    # frequency of word (chosen term)\n",
        "    else:\n",
        "      for item in found:\n",
        "        if item[0] == docId:\n",
        "          raw_count = item[1]\n",
        "          break;\n",
        "\n",
        "        # loop through position words dictionary to get list of words in a doc and count number of unique words\n",
        "        unique_words = 0\n",
        "        for key, value in position_words.items():\n",
        "            if key == docId:\n",
        "                for key, val in value.items():\n",
        "                    unique_words += 1\n",
        "\n",
        "        print(unique_words) # for testing purposes\n",
        "\n",
        "        # compliment is the term frequency in the doc minus the word we are looking for\n",
        "        compliment = unique_words - 1\n",
        "        tf = raw_count/compliment\n",
        "\n",
        "\n",
        "  # 4) Log Normalization, getting the term frequency for the given word and the given document, calling log function on that plus one\n",
        "  elif weight == 4:\n",
        "    raw = pos_ind.retrieve(word)\n",
        "    for item in raw:\n",
        "      if item[0] == docId:\n",
        "        raw_count = item[1]\n",
        "        break;\n",
        "\n",
        "    tf = math.log(1 + raw_count)\n",
        "\n",
        "  # 5) Double Normalization\n",
        "  elif weight == 5:\n",
        "\n",
        "    # if word does not exist in doc\n",
        "    found = pos_ind.retrieve(word)\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "\n",
        "    else:\n",
        "        # get raw count\n",
        "        raw = pos_ind.retrieve(word)\n",
        "        for item in raw:\n",
        "          if item[0] == docId:\n",
        "            raw_count = item[1]\n",
        "            break;\n",
        "\n",
        "        max = 0\n",
        "        # loop through position words dictionary to get max term frequency\n",
        "        for key, value in position_words.items():\n",
        "            if key == docId:\n",
        "                for inner_key, val in value.items():\n",
        "                    if inner_key != word:\n",
        "                        # print(value.items())\n",
        "                        if len(val) > max:\n",
        "                            max = len(val)\n",
        "                        # print(inner_key, len(val))\\\n",
        "\n",
        "        tf = 0.5 + (0.5 * (raw_count / max))\n",
        "\n",
        "  return tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0A-PA6JMZ0B",
        "outputId": "be6ea69c-cc73-40da-eb81-7f2619b160d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# testing Weighting Scheme function\n",
        "word = \"hello\"\n",
        "weight = 5\n",
        "docId = 1\n",
        "test = weighting(pos_ind, docId, word, weight)\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZoAWwF2MZ0C"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "#Create query vector\n",
        "def create_qv(query, positional_index):\n",
        "\n",
        "    # create an empty query vector\n",
        "    query_vector = np.empty(len(positional_index.dict_post))\n",
        "\n",
        "    index = 0\n",
        "    for word in positional_index.dict_post:\n",
        "        if word in query:\n",
        "            query_vector[index] = 1\n",
        "        else:\n",
        "            query_vector[index] = 0\n",
        "        index += 1\n",
        "\n",
        "    return query_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amcXWPurMZ0C",
        "outputId": "3aa58269-8b69-4624-82f9-8f137d6f53bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'world': <__main__.LinkedList object at 0x7b135f082a70>, 'fastest': <__main__.LinkedList object at 0x7b135f080730>, 'spider': <__main__.LinkedList object at 0x7b135f0810c0>, 'copyright': <__main__.LinkedList object at 0x7b135f081870>, '1991': <__main__.LinkedList object at 0x7b135f081180>, 'andrew': <__main__.LinkedList object at 0x7b135f0808e0>, 'varga': <__main__.LinkedList object at 0x7b135f083c10>, 'crisp': <__main__.LinkedList object at 0x7b135f080790>, 'autumn': <__main__.LinkedList object at 0x7b135f083cd0>, 'morning': <__main__.LinkedList object at 0x7b135f081600>, 'slowly': <__main__.LinkedList object at 0x7b135f080c10>, 'backed': <__main__.LinkedList object at 0x7b135f081e70>, 'driveway': <__main__.LinkedList object at 0x7b135f083400>, 'way': <__main__.LinkedList object at 0x7b135f081ab0>, 'work': <__main__.LinkedList object at 0x7b135f082530>, 'spent': <__main__.LinkedList object at 0x7b135f081ba0>, 'night': <__main__.LinkedList object at 0x7b135f080af0>, 'tree': <__main__.LinkedList object at 0x7b135f082110>, 'gargantuan': <__main__.LinkedList object at 0x7b135f081360>, 'maple': <__main__.LinkedList object at 0x7b135f082bc0>, 'car': <__main__.LinkedList object at 0x7b135f082770>, 'heavily': <__main__.LinkedList object at 0x7b135f082260>, 'laden': <__main__.LinkedList object at 0x7b135f080670>, 'multicolored': <__main__.LinkedList object at 0x7b135f082ec0>, 'red': <__main__.LinkedList object at 0x7b135f0831c0>, 'yellow': <__main__.LinkedList object at 0x7b135f0810f0>, 'orange': <__main__.LinkedList object at 0x7b135f081540>, 'leaves': <__main__.LinkedList object at 0x7b135f083280>, 'gone': <__main__.LinkedList object at 0x7b135f0817e0>, 'far': <__main__.LinkedList object at 0x7b135f081ed0>, 'thought': <__main__.LinkedList object at 0x7b135f081810>, 'noticed': <__main__.LinkedList object at 0x7b135f081f60>, 'something': <__main__.LinkedList object at 0x7b135f0828f0>, 'move': <__main__.LinkedList object at 0x7b135f082830>, 'strange': <__main__.LinkedList object at 0x7b135f0815d0>, 'toward': <__main__.LinkedList object at 0x7b135f083940>, 'across': <__main__.LinkedList object at 0x7b135f080910>, 'hood': <__main__.LinkedList object at 0x7b135f081a20>, 'surmounted': <__main__.LinkedList object at 0x7b135f081c00>, 'windshield': <__main__.LinkedList object at 0x7b135f082440>, 'see': <__main__.LinkedList object at 0x7b135f081ea0>, 'big': <__main__.LinkedList object at 0x7b135f082710>, 'ugly': <__main__.LinkedList object at 0x7b135f0837c0>, 'black': <__main__.LinkedList object at 0x7b135f082350>, 'hairy': <__main__.LinkedList object at 0x7b135f082200>, 'legs': <__main__.LinkedList object at 0x7b1351869810>, 'hate': <__main__.LinkedList object at 0x7b1351869930>, 'spiders': <__main__.LinkedList object at 0x7b1351869480>, 'right': <__main__.LinkedList object at 0x7b135186b2b0>, 'decided': <__main__.LinkedList object at 0x7b135186b8e0>, 'need': <__main__.LinkedList object at 0x7b1351869c00>, 'traveling': <__main__.LinkedList object at 0x7b135186b490>, 'companion': <__main__.LinkedList object at 0x7b135186b5b0>, 'eight': <__main__.LinkedList object at 0x7b135186b310>, 'go': <__main__.LinkedList object at 0x7b135186b7c0>, 'effort': <__main__.LinkedList object at 0x7b1351868be0>, 'persuade': <__main__.LinkedList object at 0x7b1351868c10>, 'leave': <__main__.LinkedList object at 0x7b1351868640>, 'pushed': <__main__.LinkedList object at 0x7b135186bd00>, 'wiper': <__main__.LinkedList object at 0x7b135186a3e0>, 'button': <__main__.LinkedList object at 0x7b135186ac50>, 'dope': <__main__.LinkedList object at 0x7b1351868ac0>, 'went': <__main__.LinkedList object at 0x7b1351869e40>, 'fat': <__main__.LinkedList object at 0x7b135186be50>, 'creature': <__main__.LinkedList object at 0x7b1351869ed0>, 'hung': <__main__.LinkedList object at 0x7b135186a170>, 'tenaciously': <__main__.LinkedList object at 0x7b135186a110>, 'well': <__main__.LinkedList object at 0x7b1351869150>, 'shove': <__main__.LinkedList object at 0x7b13518688b0>, 'said': <__main__.LinkedList object at 0x7b135186aa40>, 'wash': <__main__.LinkedList object at 0x7b13518683d0>, 'another': <__main__.LinkedList object at 0x7b135186bc40>, 'heavy': <__main__.LinkedList object at 0x7b135186ad10>, 'spray': <__main__.LinkedList object at 0x7b13518684c0>, 'washer': <__main__.LinkedList object at 0x7b135ea1c310>, 'solution': <__main__.LinkedList object at 0x7b135ea1e9b0>, 'momentarily': <__main__.LinkedList object at 0x7b135ea1c700>, 'blurred': <__main__.LinkedList object at 0x7b135ea1e8c0>, 'entire': <__main__.LinkedList object at 0x7b135ea1ea70>, 'rude': <__main__.LinkedList object at 0x7b135ea1e6e0>, 'driver': <__main__.LinkedList object at 0x7b135ea1ec50>, 'blasted': <__main__.LinkedList object at 0x7b135ea1cfd0>, 'horn': <__main__.LinkedList object at 0x7b135ea1ce20>, 'passed': <__main__.LinkedList object at 0x7b135ea1d4b0>, 'reported': <__main__.LinkedList object at 0x7b135ea1d720>, 'disturbing': <__main__.LinkedList object at 0x7b135ea1c3a0>, 'peace': <__main__.LinkedList object at 0x7b135ea1f160>, 'except': <__main__.LinkedList object at 0x7b135ea1e920>, 'enough': <__main__.LinkedList object at 0x7b135ea1eaa0>, 'recognize': <__main__.LinkedList object at 0x7b135ea1d630>, 'vehicle': <__main__.LinkedList object at 0x7b135ea1d510>, 'cleared': <__main__.LinkedList object at 0x7b135ea1d330>, 'gaped': <__main__.LinkedList object at 0x7b135ea1d3c0>, 'disbelief': <__main__.LinkedList object at 0x7b135ea1d210>, 'riding': <__main__.LinkedList object at 0x7b135ea1d180>, 'behind': <__main__.LinkedList object at 0x7b135ea1e1a0>, 'like': <__main__.LinkedList object at 0x7b135ea1e0e0>, 'water': <__main__.LinkedList object at 0x7b135ea1e020>, 'skier': <__main__.LinkedList object at 0x7b135ea1df60>, 'boat': <__main__.LinkedList object at 0x7b135ea1dea0>, 'quickly': <__main__.LinkedList object at 0x7b135ea1dde0>, 'swerved': <__main__.LinkedList object at 0x7b135ea1dd20>, 'left': <__main__.LinkedList object at 0x7b135ea1c9d0>, 'swear': <__main__.LinkedList object at 0x7b135ea1ca90>, 'sucker': <__main__.LinkedList object at 0x7b135ea1cb50>, 'looked': <__main__.LinkedList object at 0x7b135ea1cc10>, 'though': <__main__.LinkedList object at 0x7b135ea1e200>, 'enjoying': <__main__.LinkedList object at 0x7b135ea1eec0>, 'ride': <__main__.LinkedList object at 0x7b135ea1efe0>, 'either': <__main__.LinkedList object at 0x7b135ea1cf10>, 'muttered': <__main__.LinkedList object at 0x7b135ea1d060>, 'hard': <__main__.LinkedList object at 0x7b135ea1d2a0>, 'accelerator': <__main__.LinkedList object at 0x7b135ea1d6f0>, 'hit': <__main__.LinkedList object at 0x7b135ea1d780>, 'sixty': <__main__.LinkedList object at 0x7b135ea1da80>, 'wind': <__main__.LinkedList object at 0x7b135ea1d8a0>, 'blew': <__main__.LinkedList object at 0x7b135ea1d8d0>, 'two': <__main__.LinkedList object at 0x7b135ea1da50>, 'seventy': <__main__.LinkedList object at 0x7b135ea1db10>, 'time': <__main__.LinkedList object at 0x7b135ea1dc30>, 'eighty': <__main__.LinkedList object at 0x7b135ea1dc90>, 'five': <__main__.LinkedList object at 0x7b135ea1e2c0>, 'hanging': <__main__.LinkedList object at 0x7b135ea1e350>, 'one': <__main__.LinkedList object at 0x7b135ea1e320>, 'leg': <__main__.LinkedList object at 0x7b135ea1e500>, 'little': <__main__.LinkedList object at 0x7b135ea1e7d0>, 'history': <__main__.LinkedList object at 0x7b135ea1ebc0>, 'shouted': <__main__.LinkedList object at 0x7b135ea1ecb0>, 'triumphantly': <__main__.LinkedList object at 0x7b135ea1ed40>, 'exactly': <__main__.LinkedList object at 0x7b135ea1eda0>, 'heard': <__main__.LinkedList object at 0x7b135ea1c820>, 'siren': <__main__.LinkedList object at 0x7b135ea1f2b0>, 'old': <__main__.LinkedList object at 0x7b135ea1f250>, 'sat': <__main__.LinkedList object at 0x7b135ea1c610>, 'leered': <__main__.LinkedList object at 0x7b135ea1c250>, 'waited': <__main__.LinkedList object at 0x7b135ea1c160>, 'side': <__main__.LinkedList object at 0x7b135ea1c220>, 'road': <__main__.LinkedList object at 0x7b135ea1e9e0>, 'policeman': <__main__.LinkedList object at 0x7b135ea1e710>, 'walk': <__main__.LinkedList object at 0x7b135ea1f220>, 'know': <__main__.LinkedList object at 0x7b135ea1c850>, 'fast': <__main__.LinkedList object at 0x7b13517c7250>, 'going': <__main__.LinkedList object at 0x7b13517c7eb0>, 'asked': <__main__.LinkedList object at 0x7b1351710040>, 'began': <__main__.LinkedList object at 0x7b1351710100>, 'writing': <__main__.LinkedList object at 0x7b13517101c0>, 'ticket': <__main__.LinkedList object at 0x7b1351710280>, 'sir': <__main__.LinkedList object at 0x7b1351710340>, 'guiltily': <__main__.LinkedList object at 0x7b1351710400>, 'replied': <__main__.LinkedList object at 0x7b13517104c0>, 'speeding': <__main__.LinkedList object at 0x7b1351710580>, 'straight': <__main__.LinkedList object at 0x7b1351710640>, 'ahead': <__main__.LinkedList object at 0x7b1351710700>, 'mind': <__main__.LinkedList object at 0x7b13517107c0>, 'racing': <__main__.LinkedList object at 0x7b1351710880>, 'plausible': <__main__.LinkedList object at 0x7b1351710940>, 'answer': <__main__.LinkedList object at 0x7b1351710a00>, 'figured': <__main__.LinkedList object at 0x7b1351710ac0>, 'believe': <__main__.LinkedList object at 0x7b1351710b80>, 'real': <__main__.LinkedList object at 0x7b1351710c40>, 'reason': <__main__.LinkedList object at 0x7b1351710d00>, 'preferable': <__main__.LinkedList object at 0x7b1351710dc0>, 'breathalyzer': <__main__.LinkedList object at 0x7b1351710e80>, 'test': <__main__.LinkedList object at 0x7b1351710f40>, 'gasped': <__main__.LinkedList object at 0x7b1351711000>, 'amazement': <__main__.LinkedList object at 0x7b13517110c0>, 'evidence': <__main__.LinkedList object at 0x7b1351711180>, 'ugh': <__main__.LinkedList object at 0x7b1351711240>, 'yes': <__main__.LinkedList object at 0x7b1351711300>, 'fine': <__main__.LinkedList object at 0x7b13517113c0>, 'officer': <__main__.LinkedList object at 0x7b1351711480>, 'usually': <__main__.LinkedList object at 0x7b1351711540>, 'careful': <__main__.LinkedList object at 0x7b1351711600>, 'handed': <__main__.LinkedList object at 0x7b13517116c0>, 'pay': <__main__.LinkedList object at 0x7b1351711780>, 'courthouse': <__main__.LinkedList object at 0x7b1351711840>, 'send': <__main__.LinkedList object at 0x7b1351711900>, 'check': <__main__.LinkedList object at 0x7b13517119c0>, 'department': <__main__.LinkedList object at 0x7b1351711a80>, 'motor': <__main__.LinkedList object at 0x7b1351711b40>, 'vehicles': <__main__.LinkedList object at 0x7b1351711c00>, 'state': <__main__.LinkedList object at 0x7b1351711cc0>, 'capital': <__main__.LinkedList object at 0x7b1351711d80>, 'slow': <__main__.LinkedList object at 0x7b1351711e40>, 'drive': <__main__.LinkedList object at 0x7b1351711f00>, 'carefully': <__main__.LinkedList object at 0x7b1351711fc0>, 'waiting': <__main__.LinkedList object at 0x7b1351712080>, 'returned': <__main__.LinkedList object at 0x7b1351712140>, 'feeling': <__main__.LinkedList object at 0x7b1351712200>, 'foolish': <__main__.LinkedList object at 0x7b13517122c0>, 'muttering': <__main__.LinkedList object at 0x7b1351712380>, 'breath': <__main__.LinkedList object at 0x7b1351712440>, 'soon': <__main__.LinkedList object at 0x7b1351712500>, 'getting': <__main__.LinkedList object at 0x7b13517125c0>, 'tire': <__main__.LinkedList object at 0x7b1351712680>, 'iron': <__main__.LinkedList object at 0x7b1351712740>, 'trunk': <__main__.LinkedList object at 0x7b1351712800>, 'genuine': <__main__.LinkedList object at 0x7b13517128c0>, 'smile': <__main__.LinkedList object at 0x7b1351712980>, 'bloomed': <__main__.LinkedList object at 0x7b1351712a40>, 'face': <__main__.LinkedList object at 0x7b1351712b00>, 'watched': <__main__.LinkedList object at 0x7b1351712bc0>, 'turn': <__main__.LinkedList object at 0x7b1351712c80>, 'flashers': <__main__.LinkedList object at 0x7b1351712d40>, 'pull': <__main__.LinkedList object at 0x7b1351712e00>, 'around': <__main__.LinkedList object at 0x7b1351712ec0>, 'good': <__main__.LinkedList object at 0x7b1351712f80>, 'thing': <__main__.LinkedList object at 0x7b1351713040>, 'look': <__main__.LinkedList object at 0x7b1351713100>, 'laughing': <__main__.LinkedList object at 0x7b13517131c0>, 'hysterically': <__main__.LinkedList object at 0x7b1351713280>, 'almost': <__main__.LinkedList object at 0x7b1351713340>, 'waved': <__main__.LinkedList object at 0x7b1351713400>, 'nemesis': <__main__.LinkedList object at 0x7b13517134c0>, 'perched': <__main__.LinkedList object at 0x7b1351713580>, 'top': <__main__.LinkedList object at 0x7b1351713640>, 'hat': <__main__.LinkedList object at 0x7b1351713700>, 'cockroach': <__main__.LinkedList object at 0x7b13517137c0>, 'ate': <__main__.LinkedList object at 0x7b1351713880>, 'cincinati': <__main__.LinkedList object at 0x7b1351713940>, 'must': <__main__.LinkedList object at 0x7b1351713a00>, 'offer': <__main__.LinkedList object at 0x7b1351713ac0>, 'confession': <__main__.LinkedList object at 0x7b1351713b80>, 'movies': <__main__.LinkedList object at 0x7b1351713ca0>, 'give': <__main__.LinkedList object at 0x7b1351713d60>, 'fright': <__main__.LinkedList object at 0x7b1351713e20>, 'subject': <__main__.LinkedList object at 0x7b1351713ee0>, 'horror': <__main__.LinkedList object at 0x7b1351713fa0>, 'got': <__main__.LinkedList object at 0x7b13517140a0>, 'contented': <__main__.LinkedList object at 0x7b13517141c0>, 'may': <__main__.LinkedList object at 0x7b13517142e0>, 'call': <__main__.LinkedList object at 0x7b13517143a0>, 'ghoulish': <__main__.LinkedList object at 0x7b1351714460>, 'obsession': <__main__.LinkedList object at 0x7b1351714520>, 'get': <__main__.LinkedList object at 0x7b13517145e0>, 'chatty': <__main__.LinkedList object at 0x7b13517146a0>, 'worst': <__main__.LinkedList object at 0x7b1351714760>, 'seems': <__main__.LinkedList object at 0x7b1351714880>, 'haunting': <__main__.LinkedList object at 0x7b1351714940>, 'dreams': <__main__.LinkedList object at 0x7b1351714a00>, 'seen': <__main__.LinkedList object at 0x7b1351714ac0>, 'ghouls': <__main__.LinkedList object at 0x7b1351714b80>, 'hobgoblins': <__main__.LinkedList object at 0x7b1351714c40>, 'witches': <__main__.LinkedList object at 0x7b1351714d00>, 'moth': <__main__.LinkedList object at 0x7b1351714dc0>, 'eaten': <__main__.LinkedList object at 0x7b1351714e80>, 'werewolves': <__main__.LinkedList object at 0x7b1351714f40>, 'fangs': <__main__.LinkedList object at 0x7b1351715000>, 'creatures': <__main__.LinkedList object at 0x7b13517150c0>, 'chattered': <__main__.LinkedList object at 0x7b1351715180>, 'others': <__main__.LinkedList object at 0x7b1351715240>, 'clattered': <__main__.LinkedList object at 0x7b1351715300>, 'japanese': <__main__.LinkedList object at 0x7b13517153c0>, 'monsters': <__main__.LinkedList object at 0x7b1351715480>, 'frankenstein': <__main__.LinkedList object at 0x7b1351715540>, 'gives': <__main__.LinkedList object at 0x7b1351715600>, 'shakes': <__main__.LinkedList object at 0x7b13517156c0>, 'count': <__main__.LinkedList object at 0x7b1351715780>, 'dracula': <__main__.LinkedList object at 0x7b1351715840>, 'driving': <__main__.LinkedList object at 0x7b1351715900>, 'batty': <__main__.LinkedList object at 0x7b13517159c0>, 'par': <__main__.LinkedList object at 0x7b1351715a80>, 'oh': <__main__.LinkedList object at 0x7b1351715ba0>, 'needed': <__main__.LinkedList object at 0x7b1351715c60>, 'seltzer': <__main__.LinkedList object at 0x7b1351715d20>, 'amazing': <__main__.LinkedList object at 0x7b1351715de0>, 'much': <__main__.LinkedList object at 0x7b1351715ea0>, 'lunch': <__main__.LinkedList object at 0x7b1351715f60>, 'chew': <__main__.LinkedList object at 0x7b1351716020>, 'suburb': <__main__.LinkedList object at 0x7b13517160e0>, 'dinner': <__main__.LinkedList object at 0x7b1351716200>, 'whole': <__main__.LinkedList object at 0x7b13517162c0>, 'town': <__main__.LinkedList object at 0x7b1351716380>, 'willard': <__main__.LinkedList object at 0x7b1351716440>, 'sent': <__main__.LinkedList object at 0x7b1351716500>, 'ben': <__main__.LinkedList object at 0x7b1351716680>, 'bit': <__main__.LinkedList object at 0x7b1351716800>, 'ratty': <__main__.LinkedList object at 0x7b13517168c0>, 'half': <__main__.LinkedList object at 0x7b1351716980>, 'bad': <__main__.LinkedList object at 0x7b1351716a40>, 'scare': <__main__.LinkedList object at 0x7b1351716b00>, 'heart': <__main__.LinkedList object at 0x7b1351716bc0>, 'nearly': <__main__.LinkedList object at 0x7b1351716c80>, 'stopped': <__main__.LinkedList object at 0x7b1351716d40>, 'never': <__main__.LinkedList object at 0x7b1351716e00>, 'topped': <__main__.LinkedList object at 0x7b1351716ec0>, 'cne': <__main__.LinkedList object at 0x7b1351716f80>, 'crs': <__main__.LinkedList object at 0x7b1351717040>, 'daa79': <__main__.LinkedList object at 0x7b1351717100>, 'vdotoee': <__main__.LinkedList object at 0x7b13517171c0>, 'niee': <__main__.LinkedList object at 0x7b1351717280>, 'iuv': <__main__.LinkedList object at 0x7b1351717340>, 'ctrtg': <__main__.LinkedList object at 0x7b1351717400>, 'de': <__main__.LinkedList object at 0x7b13517174c0>, 'nataod': <__main__.LinkedList object at 0x7b1351717580>, 'file': <__main__.LinkedList object at 0x7b13517176a0>, 'downloaded': <__main__.LinkedList object at 0x7b1351717760>, 'nirvananet': <__main__.LinkedList object at 0x7b1351717820>, 'tm': <__main__.LinkedList object at 0x7b13517178e0>, 'seven': <__main__.LinkedList object at 0x7b13517179a0>, 'temple': <__main__.LinkedList object at 0x7b1351717a60>, 'screaming': <__main__.LinkedList object at 0x7b1351717b20>, 'electron': <__main__.LinkedList object at 0x7b1351717be0>, 'taipan': <__main__.LinkedList object at 0x7b1351717ca0>, 'enigma': <__main__.LinkedList object at 0x7b1351717d60>, '510': <__main__.LinkedList object at 0x7b1351717e20>, '935': <__main__.LinkedList object at 0x7b1351717ee0>, '5845': <__main__.LinkedList object at 0x7b1351717fa0>, 'burn': <__main__.LinkedList object at 0x7b135ea240a0>, 'flag': <__main__.LinkedList object at 0x7b135ea24160>, 'zardoz': <__main__.LinkedList object at 0x7b135ea24220>, '408': <__main__.LinkedList object at 0x7b135ea242e0>, '363': <__main__.LinkedList object at 0x7b135ea243a0>, '9766': <__main__.LinkedList object at 0x7b135ea24460>, 'realitycheck': <__main__.LinkedList object at 0x7b135ea24520>, 'poindexter': <__main__.LinkedList object at 0x7b135ea245e0>, 'fortran': <__main__.LinkedList object at 0x7b135ea246a0>, '527': <__main__.LinkedList object at 0x7b135ea24760>, '1662': <__main__.LinkedList object at 0x7b135ea24820>, 'lies': <__main__.LinkedList object at 0x7b135ea248e0>, 'unlimited': <__main__.LinkedList object at 0x7b135ea249a0>, 'mick': <__main__.LinkedList object at 0x7b135ea24a60>, 'freen': <__main__.LinkedList object at 0x7b135ea24b20>, '801': <__main__.LinkedList object at 0x7b135ea24be0>, '278': <__main__.LinkedList object at 0x7b135ea24ca0>, '2699': <__main__.LinkedList object at 0x7b135ea24d60>, 'new': <__main__.LinkedList object at 0x7b135ea24e20>, 'dork': <__main__.LinkedList object at 0x7b135ea24ee0>, 'sublime': <__main__.LinkedList object at 0x7b135ea24fa0>, 'biffnix': <__main__.LinkedList object at 0x7b135ea25060>, '415': <__main__.LinkedList object at 0x7b135ea25120>, '864': <__main__.LinkedList object at 0x7b135ea251e0>, 'shrine': <__main__.LinkedList object at 0x7b135ea252a0>, 'rif': <__main__.LinkedList object at 0x7b135ea25360>, 'raf': <__main__.LinkedList object at 0x7b135ea25420>, '206': <__main__.LinkedList object at 0x7b135ea254e0>, '794': <__main__.LinkedList object at 0x7b135ea255a0>, '6674': <__main__.LinkedList object at 0x7b135ea25660>, 'planet': <__main__.LinkedList object at 0x7b135ea25720>, 'mirth': <__main__.LinkedList object at 0x7b135ea257e0>, 'simon': <__main__.LinkedList object at 0x7b135ea258a0>, 'jester': <__main__.LinkedList object at 0x7b135ea25960>, '786': <__main__.LinkedList object at 0x7b135ea25a20>, '6560': <__main__.LinkedList object at 0x7b135ea25ae0>, 'raw': <__main__.LinkedList object at 0x7b135ea25ba0>, 'data': <__main__.LinkedList object at 0x7b135ea25c60>, 'nerves': <__main__.LinkedList object at 0x7b135ea25d20>, 'sits': <__main__.LinkedList object at 0x7b135ea25de0>, 'alone': <__main__.LinkedList object at 0x7b135ea25ea0>, 'abandoned': <__main__.LinkedList object at 0x7b135ea25f60>, 'friends': <__main__.LinkedList object at 0x7b135ea26020>, 'better': <__main__.LinkedList object at 0x7b135ea26140>, 'thinks': <__main__.LinkedList object at 0x7b135ea26200>, 'lot': <__main__.LinkedList object at 0x7b135ea262c0>, 'lately': <__main__.LinkedList object at 0x7b135ea26380>, 'notices': <__main__.LinkedList object at 0x7b135ea264a0>, 'things': <__main__.LinkedList object at 0x7b135ea26560>, 'lots': <__main__.LinkedList object at 0x7b135ea26680>, 'pastel': <__main__.LinkedList object at 0x7b135ea267a0>, 'colours': <__main__.LinkedList object at 0x7b135ea26860>, 'dysentery': <__main__.LinkedList object at 0x7b135ea26920>, 'colored': <__main__.LinkedList object at 0x7b135ea269e0>, 'floor': <__main__.LinkedList object at 0x7b135ea26aa0>, 'tiles': <__main__.LinkedList object at 0x7b135ea26b60>, 'maybe': <__main__.LinkedList object at 0x7b135ea26c20>, 'keep': <__main__.LinkedList object at 0x7b135ea26ce0>, 'small': <__main__.LinkedList object at 0x7b135ea26da0>, 'spark': <__main__.LinkedList object at 0x7b135ea26e60>, 'creativity': <__main__.LinkedList object at 0x7b135ea26f20>, 'alive': <__main__.LinkedList object at 0x7b135ea26fe0>, 'us': <__main__.LinkedList object at 0x7b135ea270a0>, 'thoughts': <__main__.LinkedList object at 0x7b135ea27160>, 'wander': <__main__.LinkedList object at 0x7b135ea27220>, 'take': <__main__.LinkedList object at 0x7b135ea272e0>, 'places': <__main__.LinkedList object at 0x7b135ea273a0>, 'occupy': <__main__.LinkedList object at 0x7b135ea27460>, 'space': <__main__.LinkedList object at 0x7b135ea27520>, 'people': <__main__.LinkedList object at 0x7b135ea275e0>, 'refuse': <__main__.LinkedList object at 0x7b135ea276a0>, 'poetic': <__main__.LinkedList object at 0x7b135ea277c0>, 'form': <__main__.LinkedList object at 0x7b135ea27880>, 'life': <__main__.LinkedList object at 0x7b135ea27940>, 'dead': <__main__.LinkedList object at 0x7b135ea27a00>, 'fools': <__main__.LinkedList object at 0x7b135ea27ac0>, 'dance': <__main__.LinkedList object at 0x7b135ea27b80>, 'concentric': <__main__.LinkedList object at 0x7b135ea27c40>, 'ripples': <__main__.LinkedList object at 0x7b135ea27d00>, 'waves': <__main__.LinkedList object at 0x7b135ea27dc0>, 'yeats': <__main__.LinkedList object at 0x7b135ea27e80>, 'colridge': <__main__.LinkedList object at 0x7b135ea27f40>, 'float': <__main__.LinkedList object at 0x7b135ea70040>, 'gracefully': <__main__.LinkedList object at 0x7b135ea70100>, 'minds': <__main__.LinkedList object at 0x7b135ea701c0>, 'eye': <__main__.LinkedList object at 0x7b135ea70280>, 'belong': <__main__.LinkedList object at 0x7b135ea70340>, 'man': <__main__.LinkedList object at 0x7b135ea704c0>, 'funny': <__main__.LinkedList object at 0x7b135ea70580>, 'coloured': <__main__.LinkedList object at 0x7b135ea70640>, 'suit': <__main__.LinkedList object at 0x7b135ea70700>, 'trying': <__main__.LinkedList object at 0x7b135ea707c0>, 'force': <__main__.LinkedList object at 0x7b135ea70880>, 'virtues': <__main__.LinkedList object at 0x7b135ea70940>, 'agedead': <__main__.LinkedList object at 0x7b135ea70a00>, 'masters': <__main__.LinkedList object at 0x7b135ea70ac0>, 'choking': <__main__.LinkedList object at 0x7b135ea70b80>, 'throat': <__main__.LinkedList object at 0x7b135ea70c40>, 'rises': <__main__.LinkedList object at 0x7b135ea70d00>, 'meet': <__main__.LinkedList object at 0x7b135ea70dc0>, 'sentence': <__main__.LinkedList object at 0x7b135ea70e80>, 'mouthful': <__main__.LinkedList object at 0x7b135ea70f40>, 'bile': <__main__.LinkedList object at 0x7b135ea71000>, 'ceiling': <__main__.LinkedList object at 0x7b135ea710c0>, 'holds': <__main__.LinkedList object at 0x7b135ea71180>, 'distraction': <__main__.LinkedList object at 0x7b135ea71240>, 'mathematical': <__main__.LinkedList object at 0x7b135ea71300>, 'puzzle': <__main__.LinkedList object at 0x7b135ea713c0>, 'forms': <__main__.LinkedList object at 0x7b135ea71480>, 'fractel': <__main__.LinkedList object at 0x7b135ea71540>, 'inhabit': <__main__.LinkedList object at 0x7b135ea71600>, 'non': <__main__.LinkedList object at 0x7b135ea716c0>, 'euclidean': <__main__.LinkedList object at 0x7b135ea71780>, 'temporal': <__main__.LinkedList object at 0x7b135ea71840>, 'rift': <__main__.LinkedList object at 0x7b135ea71900>, 'floats': <__main__.LinkedList object at 0x7b135ea719c0>, 'spactime': <__main__.LinkedList object at 0x7b135ea71a80>, 'primetime': <__main__.LinkedList object at 0x7b135ea71b40>, 'commercial': <__main__.LinkedList object at 0x7b135ea71c00>, 'breaks': <__main__.LinkedList object at 0x7b135ea71cc0>, 'act': <__main__.LinkedList object at 0x7b135ea71d80>, 'room': <__main__.LinkedList object at 0x7b135ea71e40>, 'giant': <__main__.LinkedList object at 0x7b135ea71f60>, 'broadcast': <__main__.LinkedList object at 0x7b135ea72020>, 'dream': <__main__.LinkedList object at 0x7b135ea720e0>, 'nbc': <__main__.LinkedList object at 0x7b135ea721a0>, 'miniseries': <__main__.LinkedList object at 0x7b135ea72260>, 'stone': <__main__.LinkedList object at 0x7b135ea72320>, 'befret': <__main__.LinkedList object at 0x7b135ea723e0>, 'bug': <__main__.LinkedList object at 0x7b135ea724a0>, 'suns': <__main__.LinkedList object at 0x7b135ea72560>, 'rise': <__main__.LinkedList object at 0x7b135ea72620>, 'west': <__main__.LinkedList object at 0x7b135ea726e0>, 'liquid': <__main__.LinkedList object at 0x7b135ea727a0>, 'languid': <__main__.LinkedList object at 0x7b135ea72860>, 'swimmers': <__main__.LinkedList object at 0x7b135ea72920>, 'precision': <__main__.LinkedList object at 0x7b135ea729e0>, 'stroke': <__main__.LinkedList object at 0x7b135ea72aa0>, 'wrestle': <__main__.LinkedList object at 0x7b135ea72b60>, 'dolphins': <__main__.LinkedList object at 0x7b135ea72c20>, 'control': <__main__.LinkedList object at 0x7b135ea72ce0>, 'remote': <__main__.LinkedList object at 0x7b135ea72da0>, 'powered': <__main__.LinkedList object at 0x7b135ea72e60>, 'civilization': <__main__.LinkedList object at 0x7b135ea72f20>, 'ever': <__main__.LinkedList object at 0x7b135ea72fe0>, 'umpteenth': <__main__.LinkedList object at 0x7b135ea73100>, 'thousand': <__main__.LinkedList object at 0x7b135ea731c0>, 'closer': <__main__.LinkedList object at 0x7b135ea732e0>, 'looks': <__main__.LinkedList object at 0x7b135ea73400>, 'dots': <__main__.LinkedList object at 0x7b135ea73520>, 'close': <__main__.LinkedList object at 0x7b135ea735e0>, 'poem': <__main__.LinkedList object at 0x7b135ea73700>, 'piece': <__main__.LinkedList object at 0x7b135ea737c0>, 'paper': <__main__.LinkedList object at 0x7b135ea73880>, 'ink': <__main__.LinkedList object at 0x7b135ea73940>, 'smeared': <__main__.LinkedList object at 0x7b135ea73a00>, 'evil': <__main__.LinkedList object at 0x7b135ea73b20>, 'unnatural': <__main__.LinkedList object at 0x7b135ea73be0>, 'tend': <__main__.LinkedList object at 0x7b135ea73ca0>, 'shy': <__main__.LinkedList object at 0x7b135ea73d60>, 'away': <__main__.LinkedList object at 0x7b135ea73e20>, 'truth': <__main__.LinkedList object at 0x7b135ea73ee0>, 'hurts': <__main__.LinkedList object at 0x7b135ea73fa0>, 'mean': <__main__.LinkedList object at 0x7b135d8d40a0>, 'philosopher': <__main__.LinkedList object at 0x7b135d8d4160>, 'head': <__main__.LinkedList object at 0x7b135d8d4220>, 'really': <__main__.LinkedList object at 0x7b135d8d42e0>, 'want': <__main__.LinkedList object at 0x7b135d8d43a0>, 'girl': <__main__.LinkedList object at 0x7b135d8d4460>, 'boyfriend': <__main__.LinkedList object at 0x7b135d8d4520>, 'cheating': <__main__.LinkedList object at 0x7b135d8d45e0>, 'telling': <__main__.LinkedList object at 0x7b135d8d4700>, 'willing': <__main__.LinkedList object at 0x7b135d8d47c0>, 'overlook': <__main__.LinkedList object at 0x7b135d8d4880>, 'discrepancies': <__main__.LinkedList object at 0x7b135d8d4940>, 'womyn': <__main__.LinkedList object at 0x7b135d8d4a00>, 'listen': <__main__.LinkedList object at 0x7b135d8d4ac0>, 'shouts': <__main__.LinkedList object at 0x7b135d8d4b80>, 'orator': <__main__.LinkedList object at 0x7b135d8d4c40>, 'guys': <__main__.LinkedList object at 0x7b135d8d4d00>, 'secrets': <__main__.LinkedList object at 0x7b135d8d4dc0>, 'beauty': <__main__.LinkedList object at 0x7b135d8d4ee0>, 'opening': <__main__.LinkedList object at 0x7b135d8d5000>, 'curtain': <__main__.LinkedList object at 0x7b135d8d50c0>, 'actors': <__main__.LinkedList object at 0x7b135d8d5180>, 'bent': <__main__.LinkedList object at 0x7b135d8d5240>, 'warped': <__main__.LinkedList object at 0x7b135d8d5300>, 'stage': <__main__.LinkedList object at 0x7b135d8d53c0>, 'twisted': <__main__.LinkedList object at 0x7b135d8d5480>, 'round': <__main__.LinkedList object at 0x7b135d8d5540>, 'poles': <__main__.LinkedList object at 0x7b135d8d5600>, 'understand': <__main__.LinkedList object at 0x7b135d8d56c0>, 'character': <__main__.LinkedList object at 0x7b135d8d5780>, 'play': <__main__.LinkedList object at 0x7b135d8d5840>, 'sees': <__main__.LinkedList object at 0x7b135d8d5960>, 'infant': <__main__.LinkedList object at 0x7b135d8d5a80>, 'eyes': <__main__.LinkedList object at 0x7b135d8d5b40>, 'blind': <__main__.LinkedList object at 0x7b135d8d5c00>, 'radiance': <__main__.LinkedList object at 0x7b135d8d5cc0>, 'fills': <__main__.LinkedList object at 0x7b135d8d5d80>, 'soul': <__main__.LinkedList object at 0x7b135d8d5e40>, 'whitelight': <__main__.LinkedList object at 0x7b135d8d5f00>, 'purefind': <__main__.LinkedList object at 0x7b135d8d5fc0>, 'productwaste': <__main__.LinkedList object at 0x7b135d8d6080>, 'machine': <__main__.LinkedList object at 0x7b135d8d6140>, 'glow': <__main__.LinkedList object at 0x7b135d8d6200>, 'innerforce': <__main__.LinkedList object at 0x7b135d8d62c0>, 'strikes': <__main__.LinkedList object at 0x7b135d8d6380>, 'match': <__main__.LinkedList object at 0x7b135d8d6440>, 'forward': <__main__.LinkedList object at 0x7b135d8d6560>, 'everything': <__main__.LinkedList object at 0x7b135d8d6620>, 'green': <__main__.LinkedList object at 0x7b135d8d66e0>, 'jello': <__main__.LinkedList object at 0x7b135d8d67a0>, 'sometimes': <__main__.LinkedList object at 0x7b135d8d6860>, 'glimpse': <__main__.LinkedList object at 0x7b135d8d6920>, '12': <__main__.LinkedList object at 0x7b135d8d6a40>, '45': <__main__.LinkedList object at 0x7b135d8d6b00>, 'sean': <__main__.LinkedList object at 0x7b135d8d6bc0>, 'gomez': <__main__.LinkedList object at 0x7b135d8d6c80>, '1993': <__main__.LinkedList object at 0x7b135d8d6da0>, 'rights': <__main__.LinkedList object at 0x7b135d8d6e60>, 'reserved': <__main__.LinkedList object at 0x7b135d8d6f20>}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ],
      "source": [
        "# testing query vector\n",
        "print(pos_ind.dict_post)\n",
        "create_qv(\"world name\", pos_ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s03B-vUQMZ0C",
        "outputId": "d58f6a88-2390-4aa8-89f8-2ca8502c037d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(len(pos_ind.retrieve(word))) #for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqE1uQcaMZ0C"
      },
      "outputs": [],
      "source": [
        "# 5\n",
        "# Calculate TF-IDF score for the query using TF-IDF matrix\n",
        "# Identify top 5 relevant documents based on the score\n",
        "\n",
        "# TF - IDF = tf * IDF\n",
        "\n",
        "def calc_tfidf(pos_ind, docId, word, weight):\n",
        "\n",
        "    # IDF(word) = log(total number of documents/ (document frequency(word) + 1))\n",
        "    idf = math.log(len(file_list) / (len(pos_ind.retrieve(word)) + 1))\n",
        "\n",
        "    # TF - IDF = tf * IDF\n",
        "    tf_idf = idf * weighting(pos_ind, docId, word, weight)\n",
        "\n",
        "    return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8RcMSzsMZ0C"
      },
      "outputs": [],
      "source": [
        "# # populate matrix with TF-IDF values\n",
        "# doc_num = len(file_list)\n",
        "# tf_weight = 1\n",
        "# tf_idf_matrix(doc_num, pos_ind, tf_weight)\n",
        "# test = calc_idf(pos_ind, 1, \"hello\", 1)\n",
        "# print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaFM8_wPMZ0D",
        "outputId": "3829659c-41ac-4025-c0df-53e50e1f386c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[None None None ... None None None]\n",
            " [None None None ... None None None]\n",
            " [None None None ... None None None]]\n"
          ]
        }
      ],
      "source": [
        "# testing building empty matrix\n",
        "matrix = tf_idf_matrix(len(file_list), pos_ind)\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "# populates TF-IDF Matrix with words in the vocbulary list\n",
        "\n",
        "def tf_idf_vals(matrix, doc_num, post_ind, weight):\n",
        "  index = 0\n",
        "\n",
        "  matrix = tf_idf_matrix(doc_num, post_ind)\n",
        "  word_ind = 0\n",
        "\n",
        "  # loop through all docIDs\n",
        "  for docID in file_list:\n",
        "\n",
        "    # loop through all words in the document\n",
        "    for item in position_words[docID]:\n",
        "\n",
        "      # limit set to prevent crash - remove before submitting / kayleigh run on VS code\n",
        "      if(word_ind < 100):\n",
        "\n",
        "        # call calc_idf function to calculate TF IDF value\n",
        "        tfidf_val = calc_idf(pos_ind, docID, item, weight)\n",
        "\n",
        "        # prints to see if function is working - TESTING PURPOSES\n",
        "        print(\"docID:\", docID)\n",
        "        print(\"word:\", item)\n",
        "        print(\"weight:\", weight)\n",
        "        print(\"tf_val:\", tfidf_val)\n",
        "\n",
        "        # assign matrix values, reduce docID index by 1\n",
        "        matrix[docID-1][word_ind] = tfidf_val\n",
        "\n",
        "        # print(\"word:\", item)\n",
        "        # # print(\"\")\n",
        "        # print(\"tf_val:\", tfidf_val)\n",
        "        # print(\"docid\", docID-1)\n",
        "        # print(\"word_ind:\", word_ind)\n",
        "        # print(\"\")\n",
        "\n",
        "      # increment word index\n",
        "      word_ind += 1\n",
        "\n",
        "\n",
        "  return matrix\n"
      ],
      "metadata": {
        "id": "R4DGQTv_epXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSsdGvPaMZ0D",
        "outputId": "bee61626-a541-4d87-a26c-afecac793c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docID: 1\n",
            "word: world\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: fastest\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: spider\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: copyright\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: 1991\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: andrew\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: varga\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: crisp\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: autumn\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: morning\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: slowly\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: backed\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: driveway\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: way\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: work\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: spent\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: night\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: tree\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: gargantuan\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: maple\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: car\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: heavily\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: laden\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: multicolored\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: red\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: yellow\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: orange\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: leaves\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: gone\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: far\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: thought\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: noticed\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: something\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: move\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: strange\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: toward\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: across\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: hood\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: surmounted\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: windshield\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: see\n",
            "weight: 1\n",
            "tf_val: -0.2876820724517809\n",
            "docID: 1\n",
            "word: big\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: ugly\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: black\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: hairy\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: legs\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: hate\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: spiders\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: right\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: decided\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: need\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: traveling\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: companion\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: eight\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: go\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: effort\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: persuade\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: leave\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: pushed\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: wiper\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: button\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: dope\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: went\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: fat\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: creature\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: hung\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: tenaciously\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: well\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: shove\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: said\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: wash\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: another\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: heavy\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: spray\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: washer\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: solution\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: momentarily\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: blurred\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: entire\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: rude\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: driver\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: blasted\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: horn\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: passed\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: reported\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: disturbing\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: peace\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: except\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: enough\n",
            "weight: 1\n",
            "tf_val: 0.0\n",
            "docID: 1\n",
            "word: recognize\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: vehicle\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: cleared\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: gaped\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: disbelief\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: riding\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: behind\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: like\n",
            "weight: 1\n",
            "tf_val: -0.2876820724517809\n",
            "docID: 1\n",
            "word: water\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: skier\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "docID: 1\n",
            "word: boat\n",
            "weight: 1\n",
            "tf_val: 0.4054651081081644\n",
            "[[0.4054651081081644 0.4054651081081644 0.4054651081081644 ... None None\n",
            "  None]\n",
            " [None None None ... None None None]\n",
            " [None None None ... None None None]]\n"
          ]
        }
      ],
      "source": [
        "# testing populating matrix with vocabulary words\n",
        "# print(pos_ind.retrieve(\"hello\"))\n",
        "#doc, freq, pos\n",
        "# print(pos_ind.items())\n",
        "\n",
        "# for key, value in pos_ind:\n",
        "  # print(key)\n",
        "\n",
        "# for key, value in pos_ind:\n",
        "  # print(key)\n",
        "\n",
        "# for item in file_list:\n",
        "#   print(item)\n",
        "\n",
        "# test populating TF IDF matrix\n",
        "\n",
        "vals = tf_idf_vals(matrix, len(file_list), pos_ind, 1)\n",
        "print(vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq9Sb68tMZ0D",
        "outputId": "b252c22f-0c97-46a8-edae-ef3691213acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'spider.txt', 2: 'tctac.txt', 3: 'glimpse1.txt'}\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "print(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = calc_idf(pos_ind, 1, \"boat\", 1)\n",
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrpQ9LSLnY23",
        "outputId": "6bbc401d-4126-4326-a61d-dd0902226439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4054651081081644"
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VmiFLiiCrboh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}