{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cDMmlnxjkVj"
      },
      "source": [
        "# CP423: Assignment 2\n",
        "\n",
        "Group Number: 5\n",
        "\n",
        "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1shKdZwjkVk"
      },
      "outputs": [],
      "source": [
        "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
        "# pip install certifi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEzvPUiOjkVl"
      },
      "outputs": [],
      "source": [
        "import certifi\n",
        "import ssl\n",
        "import os\n",
        "\n",
        "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TeuJxAojkVl",
        "outputId": "0ed3b45b-9cae-4b1c-f640-46cbe6c01014"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/kayleighhabib/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/kayleighhabib/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# inputs\n",
        "import os\n",
        "import re\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpo3UmvIjkVm"
      },
      "source": [
        "### Question 1: Positional Index\n",
        "\n",
        "a. Execute the subsequent preprocessing tasks on the provided dataset:\n",
        "\n",
        "1. Transform the text to lowercase\n",
        "\n",
        "2. Perform word tokenization\n",
        "\n",
        "3. Eliminate stopwords from the tokens\n",
        "\n",
        "4. Remove punctuation marks from the tokens\n",
        "\n",
        "5. Eliminate empty space tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caqZhEmOjkVm"
      },
      "outputs": [],
      "source": [
        "# used to clean the files\n",
        "def text_preprocessing_file(file):\n",
        "\n",
        "    # reading the file line by line\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # 1. converting to lowercase\n",
        "    lines = [x.lower() for x in lines] # list\n",
        "\n",
        "    # doing this first as  the positional dictionary includes the punctuation but then when trying to match the query will not work\n",
        "    # 4. Remove punctuation / special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9\n",
        "    alphanumerictxt = []\n",
        "\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in lines:\n",
        "        stripped = re.sub(pattern,\" \", i).strip()\n",
        "        split_stripped = stripped.split() # list\n",
        "        for j in split_stripped:\n",
        "            alphanumerictxt.append(j)\n",
        "\n",
        "    # join all the lines into one string\n",
        "    data = \" \".join(alphanumerictxt)\n",
        "\n",
        "    # 2. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "\n",
        "    # 3. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in tokenization:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "\n",
        "    # 5. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in stop_words:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "\n",
        "    # store the dictionary of positions / location for each word\n",
        "    dict_position = {} # dictionary has a word and the list of the position\n",
        "    position_counter = 1\n",
        "\n",
        "    # traverse through the list of all words\n",
        "    for word in updated_list:\n",
        "        # everytime it finds a new word it will create a new key and create a list for the position\n",
        "        if word not in dict_position:\n",
        "            dict_position[word] = [position_counter]\n",
        "        else: # the word already exists so just get the position it is found in\n",
        "             dict_position[word].append(position_counter)\n",
        "        position_counter+=1\n",
        "\n",
        "\n",
        "    # gets the positional index for the final list\n",
        "    dict_final = {}\n",
        "    for element in dict_position:\n",
        "        dict_final[element] = dict_position[element]\n",
        "\n",
        "    return dict_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH56BO7SjkVn"
      },
      "outputs": [],
      "source": [
        "# Copied from A1\n",
        "# used to clean the user query\n",
        "def text_preprocessing_query(query):\n",
        "\n",
        "    # a. converting to lowercase\n",
        "    lines = [x.lower() for x in query]\n",
        "\n",
        "    # join all the lines into one string\n",
        "    data = \"\".join(lines)\n",
        "\n",
        "    # b. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "\n",
        "    # remove duplicates - less words to do the next steps with\n",
        "    newtoken = list(dict.fromkeys(tokenization))\n",
        "\n",
        "    # c. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in newtoken:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "    # d. Remove special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
        "    alphanumerictxt = []\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in stop_words:\n",
        "        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n",
        "\n",
        "    # e. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in alphanumerictxt:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "    # after all the other steps are completed check again to make sure no duplicated values\n",
        "    final_list = list(dict.fromkeys(updated_list))\n",
        "\n",
        "\n",
        "    return final_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phIlvqHSjkVn"
      },
      "outputs": [],
      "source": [
        "# import and read data files\n",
        "def readfile_fun():\n",
        "  # takes the names of the files in the folder and creates a list\n",
        "  file_names = os.listdir(\"./data\")\n",
        "\n",
        "  # file id (key)\n",
        "  counter = 1\n",
        "\n",
        "  #dictionary1 - gets the list of words, frequency and position\n",
        "  # i.e : {\"pyramid\" : 4 [1,3,5,7]}\n",
        "  dict_words = {}\n",
        "  #dictionary2 - file dictionary (id to file name)\n",
        "  # i.e : 0 = \"hole2nar.txt\"\n",
        "  dict_files = {}\n",
        "\n",
        "  # goes through each file in the list of file names\n",
        "  for i in file_names:\n",
        "    # gets the working directory (this may be different for everyone)\n",
        "    file_path = os.getcwd() + \"/data/\" + i\n",
        "    # opens the file in reading mode\n",
        "    file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
        "    # calls the function\n",
        "    # words is a dictionary of the word and the positions\n",
        "    words = text_preprocessing_file(file)\n",
        "\n",
        "    # updates the dictionaries\n",
        "    dict_words[counter] = words # dictionary of dictionary\n",
        "    dict_files[counter] = i # list of all the file names\n",
        "\n",
        "    # increment the counter\n",
        "    counter+=1\n",
        "\n",
        "    # closes the file\n",
        "    file.close()\n",
        "\n",
        "  return dict_words, dict_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsQk4k5ijkVn"
      },
      "source": [
        "### Question 1: Positional Index\n",
        "b. Develop the positional index data structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVUPNpV_jkVn"
      },
      "outputs": [],
      "source": [
        "#b.\n",
        "\n",
        "# created a linked list class to store all the positions for the words\n",
        "# created a node class to get pointer values\n",
        "\n",
        "class Node:\n",
        "    # value: docID, count and list of position\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.next = None\n",
        "\n",
        "class LinkedList:\n",
        "\n",
        "    # initialize linked list\n",
        "    def __init__(self):\n",
        "        self._front = None\n",
        "        self._rear = None\n",
        "        self._counter = 0\n",
        "\n",
        "    # key is the word, value is the docID\n",
        "    # insert to the beginning\n",
        "    def insert_front(self, value):\n",
        "        # if it is an empty list\n",
        "        newnode = Node(value)\n",
        "        if self._front is None:\n",
        "            self._front = newnode\n",
        "            self._rear = newnode\n",
        "        else:\n",
        "            newnode.next = self._front\n",
        "            self._front = newnode\n",
        "        self._counter +=1\n",
        "        return\n",
        "\n",
        "    # insert to the end\n",
        "    def insert_rear(self, value):\n",
        "        # if it is an empty list\n",
        "        newnode = Node(value)\n",
        "        if self._front is None:\n",
        "            self._front = newnode\n",
        "            self._rear = newnode\n",
        "        else:\n",
        "            self._rear.next = newnode\n",
        "            self._rear = newnode\n",
        "        self._counter +=1\n",
        "        return\n",
        "\n",
        "    # for testing purposes - shows us the string representation of the linked list\n",
        "    def printList(self):\n",
        "        temp = self._front\n",
        "        while temp is not None:\n",
        "            print(temp.value)\n",
        "            temp = temp.next\n",
        "        return\n",
        "\n",
        "    # creates a list\n",
        "    def merge(self):\n",
        "        merged_list = []\n",
        "        temp = self._front\n",
        "        while temp is not None:\n",
        "            merged_list.append(temp.value)\n",
        "            temp = temp.next\n",
        "        return merged_list\n",
        "\n",
        "    # to print the required output\n",
        "    def __str__(self) -> str:\n",
        "        temp = self._front\n",
        "        newstring = \"\"\n",
        "        while temp is not None:\n",
        "            newstring += \" DocID: \"+ str(temp.value[0])\n",
        "            newstring += \", Frequency: \"+ str(temp.value[1])\n",
        "            newstring += \", Position: \"+ str(temp.value[2])+ \"\\n\"\n",
        "            temp = temp.next\n",
        "        return newstring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJOOzPphjkVn"
      },
      "outputs": [],
      "source": [
        "# created a position Index class to get a dictionary of the words, the frequency and positions\n",
        "class PositionIndex:\n",
        "    def __init__(self):\n",
        "        self.dict_post = {} # key = word, value = linked list\n",
        "\n",
        "    # key = word\n",
        "    def insert(self, key, docID, position):\n",
        "        # order in which the list is created (document number, number of occurrences and all the positions)\n",
        "        newlist = [docID, len(position), position]\n",
        "        if key not in self.dict_post: # if word is not already in the dictionary\n",
        "            self.dict_post[key] = LinkedList() # created a new linked list for a new word\n",
        "        self.dict_post[key].insert_rear(newlist) # adds to the existing list\n",
        "        return\n",
        "\n",
        "    # takes one word and returns all the documents and positions\n",
        "    def retrieve(self, key):\n",
        "        ret = []\n",
        "        if key in self.dict_post: # if key is found\n",
        "            ret = self.dict_post[key].merge() # return the entire linked list as a list\n",
        "        return ret\n",
        "\n",
        "    def retrieve_doc(self, key, docID):\n",
        "        #takes a list of docIDs and checks if that key is there - goes to the linked list and returns all the linked list of only the doc ids that match (sub linked list in that word)\n",
        "        doc_list = []\n",
        "        if key in self.dict_post: # if the key is in the dictionary\n",
        "            word_list = self.dict_post[key].merge() # has the doc ids and positions\n",
        "            for doc in word_list:  # for every documentid in the list\n",
        "                if doc[0] == docID: # if the document id are the same\n",
        "                    doc_list.append(doc) # get the linked list (the word and position)\n",
        "        return doc_list\n",
        "\n",
        "    # stringify function\n",
        "    def __str__(self) -> str:\n",
        "        newstring = \"\" # prints the output\n",
        "        for key in self.dict_post.keys():\n",
        "            newstring+= \"{ \" + key # key = word\n",
        "            newstring+= \": (\"+ str(self.dict_post[key]._counter) + \" {\\n\" # frequency\n",
        "            newstring+= str(self.dict_post[key]) #positions\n",
        "            newstring+= \"} ) } \\n\\n\"\n",
        "\n",
        "        return newstring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf8A6DCcjkVo"
      },
      "outputs": [],
      "source": [
        "# copied from A1\n",
        "# creates the inverted index to get the doc ids\n",
        "class HashInvertedIndex:\n",
        "\n",
        "    # initialize hash table\n",
        "    def __init__(self, capacity=100): # manually set the capacity\n",
        "        self.capacity = capacity\n",
        "        self.table = [[] for _ in range(capacity)]\n",
        "        self.count = 0\n",
        "\n",
        "    # finding the corresponding value for the key\n",
        "    def _index(self, word):\n",
        "        index = hash(word) % self.capacity\n",
        "        return index\n",
        "\n",
        "    # add key-value pair to the hash table\n",
        "    def insert(self, key, value):\n",
        "\n",
        "        # compute the hash index for the word\n",
        "        hash_index = self._index(key)\n",
        "        for i, kv in enumerate(self.table[hash_index]):\n",
        "            k, v = kv\n",
        "            if k == key:\n",
        "                # add the value to the existing set of values\n",
        "                v.add(value)\n",
        "                # add the entry to the table\n",
        "                self.table[hash_index][i] = (k,v)\n",
        "                return\n",
        "        self.table[hash_index].append((key, {value}))\n",
        "\n",
        "    # retrieve the set of values for a given key, if not found, returns an empty set\n",
        "    def retrieve(self, key):\n",
        "        # compute the hash index for a given key\n",
        "        hash_index = self._index(key)\n",
        "        for k, v in self.table[hash_index]:\n",
        "            # if the key if found, return the set\n",
        "            if k == key:\n",
        "                return v\n",
        "        return set()\n",
        "\n",
        "    # for testing purposes - shows us the string representation of the hash tables\n",
        "    def __rep__(self):\n",
        "        return str(self.table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIxpueBWjkVo"
      },
      "outputs": [],
      "source": [
        "# build the positional index - reads the documents and their corresponding words\n",
        "def create_positional_inverted_index():\n",
        "\n",
        "    # initialize the  linked list / positional index\n",
        "    positional_index = PositionIndex()\n",
        "\n",
        "    # calls the function above\n",
        "    # returns list of words in doc and list of docs\n",
        "    words, files = readfile_fun()\n",
        "\n",
        "    # fill in the inverted index\n",
        "    # key = file IDs\n",
        "    # value = dictionary of words\n",
        "    for fileID, value in words.items():\n",
        "        for word in value.keys():\n",
        "            positional_index.insert(word, fileID, value[word])\n",
        "\n",
        "    return positional_index, files, words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUIYTvm3jkVo"
      },
      "outputs": [],
      "source": [
        "# Copied from A1\n",
        "# build the inverted index - reads the documents and their corresponding words\n",
        "def create_inverted_index():\n",
        "\n",
        "    # initialize the inverted index\n",
        "    inverted_index = HashInvertedIndex()\n",
        "\n",
        "    # calls the function above\n",
        "    # returns list of words in doc and list of docs\n",
        "    words_inv,files = readfile_fun()\n",
        "    # fill in the inverted index\n",
        "    for key, value in words_inv.items():\n",
        "        for word in value.keys():\n",
        "            inverted_index.insert(word, key) # inserting counter instead of doc name\n",
        "\n",
        "    return inverted_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7A-iEXqjkVo"
      },
      "source": [
        "### Question 1: Positional Index\n",
        "\n",
        "c. Facilitate the searching of phrase queries, assuming the query length is equal to or less than 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8wf1GimjkVo"
      },
      "outputs": [],
      "source": [
        "# C\n",
        "# function will return all the documents that the words in the query are found\n",
        "def find_common_docs(query, InvertedIndex):\n",
        "  index = 0\n",
        "  word = query[index] # setting word to the first position\n",
        "  doc_list = InvertedIndex.retrieve(word) # retrieves the documentID\n",
        "\n",
        "  #loop through array of operations\n",
        "  while (index < len(query)-1):\n",
        "  # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the Inverted Index\n",
        "    next_word = query[index+1] # gets the next word\n",
        "    list1 = InvertedIndex.retrieve(next_word) # gets the documents that correspond to the next word\n",
        "    doc_list = doc_list.intersection(list1) # takes the intersection --> all that are the same\n",
        "    index+=1 # increment index until the end of the query is reached\n",
        "\n",
        "  return doc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zgxqp-1jkVo"
      },
      "outputs": [],
      "source": [
        "# function will return all the documents where the positions are next to each other like in the query\n",
        "\n",
        "# passing in a list which includes all the words in the query, docIDs, frequency and position\n",
        "# position_words = dictionary, dictionary, linked list\n",
        "def find_positions(query, pos_ind, common_doc):\n",
        "    # create new_dictionary to store the words and the position dictionary\n",
        "    new_dict={}\n",
        "    new_list = [] # list of all the positions\n",
        "    adj_list = [] # positions are adjusted for the query\n",
        "    count = len(query) # how many words in the query\n",
        "    i = 0\n",
        "\n",
        "    # get the common documents and positions\n",
        "    for word in query:\n",
        "        new_dict[word] = pos_ind.retrieve_doc(word, common_doc) # words with only common document with the positions\n",
        "\n",
        "    for word in query:\n",
        "        new_list.append(new_dict[word][0][2]) # pull out the positional list (at index 2)\n",
        "\n",
        "    # shifts the positions of the words to get them to be the same\n",
        "    for i in range (count):\n",
        "        another_list= new_list[i] # for the first word get the positions\n",
        "        temp_list = [x + count - 1 - i for x in another_list] # for every element in the list going to add the specific value and put into a new list\n",
        "        # i. e: query length is 5, first position will get i+4 next will get i+3 etc\n",
        "        adj_list.append(temp_list)\n",
        "\n",
        "    # gets the intersection\n",
        "    i = 0\n",
        "    intersected_list = adj_list[i] # set the start of the intersected list to be the list of positions of the first document\n",
        "    i+=1\n",
        "    for i in range (count):\n",
        "        # gets the next position\n",
        "        next_list = adj_list[i]\n",
        "        # since all the positions are adjusted to be the same take the intersection to get where it is equal\n",
        "        intersected_list = set(intersected_list).intersection(next_list)\n",
        "\n",
        "    #unshift the positions to be able to print the start\n",
        "    final_list = [x - (count - 1) for x in intersected_list]\n",
        "\n",
        "    return final_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG2Hk-e0jkVo"
      },
      "source": [
        "### Question 1: Positional Index\n",
        "\n",
        "Calling everything from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Yhf0-tjkVo"
      },
      "outputs": [],
      "source": [
        "# calls the specific functions\n",
        "# takes a couple seconds to run\n",
        "pos_ind,file_list,position_words = create_positional_inverted_index()\n",
        "inverted_index = create_inverted_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtybvwIOjkVo"
      },
      "outputs": [],
      "source": [
        "# print(file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvJkS4UUjkVp",
        "outputId": "f5797e5d-1761-4b3e-9d82-1da9e1ff20f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1 entered (cleaned):  ['sherlock', 'holmes']\n",
            "Document name:  empty.txt\n",
            "The query start positions:  [3235, 133, 911, 4081, 185]\n",
            "Document name:  veiledl.txt\n",
            "The query start positions:  [25, 533]\n",
            "Document name:  3student.txt\n",
            "The query start positions:  [96, 30]\n",
            "Document name:  3gables.txt\n",
            "The query start positions:  [25, 2715, 421]\n",
            "Document name:  bulmrx.txt\n",
            "The query start positions:  [607]\n",
            "Document name:  enginer.txt\n",
            "The query start positions:  [582, 647, 3087, 3185, 26]\n",
            "Document name:  goldenp.txt\n",
            "The query start positions:  [983, 3010, 1357, 990]\n",
            "Document name:  missing.txt\n",
            "The query start positions:  [1956, 649, 1388, 44, 846, 214]\n",
            "Document name:  musgrave.txt\n",
            "The query start positions:  [26, 396]\n",
            "Document name:  5orange.txt\n",
            "The query start positions:  [2497, 2754, 153, 2055, 3016, 1994, 1617, 241, 310, 25]\n",
            "Document name:  silverb.txt\n",
            "The query start positions:  [160, 3842, 2852, 3432]\n",
            "Document name:  hound-b.txt\n",
            "The query start positions:  [22802, 21, 24, 920, 11672, 10145, 3111, 22701, 4275, 22580, 22966, 7353, 16959, 6081, 9669, 7497, 22473, 718, 21076, 7509, 7385, 15705, 21468, 5215, 872, 7275, 7660, 1904, 4467, 500, 10106, 7036, 2429]\n",
            "Document name:  bruce-p.txt\n",
            "The query start positions:  [4207, 4903, 4298]\n",
            "Document name:  6napolen.txt\n",
            "The query start positions:  [864, 35, 1509, 3344, 2289, 2066, 2388, 3193, 2907]\n",
            "Document name:  wisteria.txt\n",
            "The query start positions:  [1653, 3364, 581, 1014]\n",
            "Document name:  abbey.txt\n",
            "The query start positions:  [2267]\n",
            "Document name:  blackp.txt\n",
            "The query start positions:  [3119, 2618, 1270, 150]\n",
            "Document name:  shoscomb.txt\n",
            "The query start positions:  [23, 2390]\n",
            "Document name:  solitary.txt\n",
            "The query start positions:  [2857, 27, 1541]\n",
            "Document name:  holmesbk.txt\n",
            "The query start positions:  [1535, 2051, 516, 520, 2056, 1548, 2573, 2582, 1560, 2081, 43, 556, 2603, 2608, 565, 3130, 1093, 2119, 584, 2635, 3150, 3157, 2139, 1631, 97, 2662, 104, 2668, 110, 624, 2674, 116, 2165, 3189, 2680, 634, 122, 3195, 1150, 1664, 3200, 130, 643, 142, 659, 1683, 149, 3222, 1178, 156, 668, 2208, 3232, 1191, 168, 2730, 1195, 2737, 3253, 183, 2231, 2745, 2235, 2749, 2241, 1735, 2253, 1742, 2765, 1238, 218, 733, 2269, 224, 1250, 2787, 230, 1256, 1768, 2289, 1266, 1786, 251, 257, 1285, 1291, 273, 1299, 2325, 279, 1304, 1819, 284, 1309, 1826, 1315, 2853, 1318, 2347, 1324, 1837, 815, 1330, 826, 1342, 833, 1348, 2885, 1355, 2379, 2893, 1362, 853, 1880, 2393, 2908, 1374, 2398, 1381, 358, 2917, 874, 1386, 2410, 368, 1394, 1401, 890, 378, 1405, 2947, 1419, 909, 2448, 405, 1433, 1439, 928, 417, 2980, 428, 942, 1454, 2991, 2999, 1471, 3014, 967, 1991, 1481, 2504, 3020, 1487, 2512, 3026, 1492, 2004, 2517, 3032, 2524, 990, 3038, 482, 996, 1509, 3045, 1519, 2553, 1530]\n",
            "Number of matching documents:  20\n",
            "File name(s):  ['empty.txt', 'veiledl.txt', '3student.txt', '3gables.txt', 'bulmrx.txt', 'enginer.txt', 'goldenp.txt', 'missing.txt', 'musgrave.txt', '5orange.txt', 'silverb.txt', 'hound-b.txt', 'bruce-p.txt', '6napolen.txt', 'wisteria.txt', 'abbey.txt', 'blackp.txt', 'shoscomb.txt', 'solitary.txt', 'holmesbk.txt']\n"
          ]
        }
      ],
      "source": [
        "# ask user to input sentence\n",
        "sentence = input(\"Enter your query (up to 5 words) : \" )\n",
        "# calls the function at the beginning to clean the query\n",
        "cleaned_sentence = text_preprocessing_query(sentence)\n",
        "# prints the cleaned sentence so user can see what is happening\n",
        "print(\"Sentence 1 entered (cleaned): \" , cleaned_sentence)\n",
        "index = 1\n",
        "# looping through in case user enters more than 5 words (more than 5 after the preprocessing)\n",
        "while len(cleaned_sentence) > 5:\n",
        "    index+= 1\n",
        "    sentence = input(\"Enter your query (up to 5 words) : \" )\n",
        "    cleaned_sentence = text_preprocessing_query(sentence)\n",
        "    print(\"Sentence%2d entered (cleaned): \"% index, cleaned_sentence)\n",
        "\n",
        "file_names = []\n",
        "if len(cleaned_sentence) > 0:\n",
        "    # finds all common documents by callimg function from part c\n",
        "    common_documents = find_common_docs(cleaned_sentence, inverted_index)\n",
        "    common_documents = list(common_documents) # creates a list of all the common documents\n",
        "    count = 0\n",
        "    # for every document in the common list\n",
        "    for doc in common_documents:\n",
        "        final = find_positions(cleaned_sentence, pos_ind, doc) # find the positions\n",
        "        if len(final)!= 0:\n",
        "            file_names.append(file_list[doc]) # creates a copy of the files names\n",
        "            print(\"Document name: \",file_list[doc]) # prints the file name\n",
        "            print(\"The query start positions: \", final) # prints the start of the query in the text file that the query matches\n",
        "            count+=1\n",
        "    print(\"Number of matching documents: \", count) # prints the number of matches\n",
        "    print(\"File name(s): \",file_names) # prints the list of file names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMX-fEKWjkVp"
      },
      "source": [
        "### Question 2: Ranking and Term-Weighting\n",
        "\n",
        "1. First, apply the preprocessing steps mentioned in Q1 to the given dataset:\n",
        "- Convert the text to lowercase.\n",
        "- Perform word tokenization.\n",
        "- Remove stopwords from the tokens.\n",
        "- Remove punctuation marks from the tokens.\n",
        "- Remove blank space tokens.\n",
        "2. Construct the matrix with dimensions (number of documents) x (vocabulary size).\n",
        "3. Populate the TF-IDF values in the matrix for each word in the vocabulary.\n",
        "4. Create a query vector with a size equal to the vocabulary.\n",
        "5. Calculate the TF-IDF score for the query using the TF-IDF matrix. Identify the top\n",
        "5 relevant documents based on the score.\n",
        "6. Apply all 5 weighting schemes for term frequency calculation and provide the TF-\n",
        "IDF score and results for each scheme individually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU8uhFd8jkVp"
      },
      "outputs": [],
      "source": [
        "#2 creates a matrix of just 0s\n",
        "def tf_idf_matrix(doc_num, pos_ind):\n",
        "  rows = doc_num\n",
        "  cols = len(pos_ind.dict_post)\n",
        "  matrix = np.zeros((rows, cols), dtype = object).reshape(rows, cols)\n",
        "\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jaw4h8SjkVp"
      },
      "outputs": [],
      "source": [
        "# Weighting Schemes\n",
        "def weighting(pos_ind, docId, word, weight):\n",
        "  tf = 0\n",
        "\n",
        "  # 1) Binary, if word exists in documents returns 1, else 0\n",
        "  if weight == 1:\n",
        "    found = pos_ind.retrieve(word) # finds list of documents that has word\n",
        "    for i in found: # i is a list, first element is docid\n",
        "      if i[0] == docId:\n",
        "        tf = 1\n",
        "        break;\n",
        "\n",
        "  # 2) Raw Count, getting the term frequency for the given word in the given document\n",
        "  elif weight == 2:\n",
        "    raw = pos_ind.retrieve(word)\n",
        "    for item in raw:\n",
        "      if item[0] == docId:\n",
        "        tf = item[1]\n",
        "        break;\n",
        "\n",
        "    # 3) Term Frequency, dividing raw count by compiment of chosen term\n",
        "  elif weight == 3:\n",
        "    found = pos_ind.retrieve(word)\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "\n",
        "    # frequency of word (chosen term)\n",
        "    else:\n",
        "      for item in found:\n",
        "        if item[0] == docId:\n",
        "          raw_count = item[1]\n",
        "          # print(raw_count)\n",
        "          break;\n",
        "\n",
        "        else:\n",
        "          raw_count = 0\n",
        "\n",
        "      # loop through position words dictionary to get list of words in a doc and count number of unique words\n",
        "      unique_words = 0\n",
        "\n",
        "      for key, value in position_words.items():\n",
        "        if key == docId:\n",
        "            for key, val in value.items():\n",
        "                unique_words += 1\n",
        "\n",
        "      # print(unique_words) # for testing purposes\n",
        "\n",
        "      # compliment is the term frequency in the doc minus the word we are looking for\n",
        "      compliment = unique_words - 1\n",
        "      tf = raw_count/compliment\n",
        "\n",
        "  # 4) Log Normalization, getting the term frequency for the given word and the given document, calling log function on that plus one\n",
        "  elif weight == 4:\n",
        "    raw = pos_ind.retrieve(word)\n",
        "    for item in raw:\n",
        "      if item[0] == docId:\n",
        "        raw_count = item[1]\n",
        "        break;\n",
        "\n",
        "    tf = math.log(1 + raw_count)\n",
        "\n",
        "  # 5) Double Normalization\n",
        "  elif weight == 5:\n",
        "    raw_count = 0\n",
        "\n",
        "    # if word does not exist in doc\n",
        "    found = pos_ind.retrieve(word)\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "\n",
        "    else:\n",
        "        # get raw count\n",
        "        raw = pos_ind.retrieve(word)\n",
        "        for item in raw:\n",
        "          if item[0] == docId:\n",
        "            raw_count = item[1]\n",
        "            break;\n",
        "\n",
        "        max = 0\n",
        "        # loop through position words dictionary to get max term frequency\n",
        "        for key, value in position_words.items():\n",
        "            if key == docId:\n",
        "              for inner_key, val in value.items():\n",
        "                if inner_key != word:\n",
        "                  # print(value.items())\n",
        "                    if len(val) > max:\n",
        "                      max = len(val)\n",
        "                      # print(inner_key, len(val))\\\n",
        "\n",
        "        tf = 0.5 + (0.5 * (raw_count / max))\n",
        "\n",
        "  return tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28AdnB1ljkVp"
      },
      "outputs": [],
      "source": [
        "# # doesnt work for 3\n",
        "# for key in position_words:\n",
        "#     docID = key\n",
        "#     a = weighting(pos_ind,docID,\"hole\",3)\n",
        "#     print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oU3eRWXjkVp"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "# Create query vector\n",
        "# Creates the matrix for all the words in dictionary\n",
        "def create_qv(query, positional_index):\n",
        "    # create an empty query vector\n",
        "    query_vector = np.zeros(len(positional_index.dict_post))\n",
        "    vocab_list = positional_index.dict_post.keys() # list of all the words\n",
        "    index = 0\n",
        "    for word in vocab_list:\n",
        "        if word in query:\n",
        "            query_vector[index] = 1\n",
        "        else:\n",
        "            query_vector[index] = 0\n",
        "        index += 1\n",
        "\n",
        "    return query_vector, vocab_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJgQxhnxjkVp"
      },
      "outputs": [],
      "source": [
        "# query_vector ,vocab_list = create_qv(\"hole\", pos_ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxHMfw5ajkVp"
      },
      "outputs": [],
      "source": [
        "# 5\n",
        "# Calculate TF-IDF score for the all the words in the list using TF-IDF matrix\n",
        "# Identify top 5 relevant documents based on the score\n",
        "\n",
        "# TF - IDF = tf * IDF\n",
        "\n",
        "def calc_tfidf(pos_ind, docId, word, weight):\n",
        "\n",
        "    # IDF(word) = log(total number of documents/ (document frequency(word) + 1))\n",
        "    idf = math.log(len(file_list) / (len(pos_ind.retrieve(word)) + 1))\n",
        "\n",
        "    # TF - IDF = tf * IDF\n",
        "    tf_idf = idf * weighting(pos_ind, docId, word, weight) # returns the score\n",
        "\n",
        "    return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmjmydxajkVp"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "# populates TF-IDF Matrix with words in the vocbulary list\n",
        "\n",
        "def tf_idf_vals(matrix, doc_num, post_ind, weight, vocab_list):\n",
        "\n",
        "  # create matrix of size (document numbers x vocabulary list)\n",
        "  matrix = tf_idf_matrix(doc_num, post_ind)\n",
        "\n",
        "  for key in position_words.keys():\n",
        "    docID = key\n",
        "    word_ind = 0\n",
        "    for word in vocab_list:  # go through all the words\n",
        "      tfidf_val = calc_tfidf(pos_ind, docID, word, weight) # fill it in with the values\n",
        "      matrix[docID-1][word_ind] = tfidf_val #making the matrix\n",
        "      # print(matrix[docID-1][word_ind]) #prints\n",
        "      word_ind+=1\n",
        "\n",
        "  return matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnckJ3B_jkVq"
      },
      "outputs": [],
      "source": [
        "# matrix = tf_idf_matrix(len(file_list), pos_ind) #blank matix for the specific word\n",
        "# result= tf_idf_vals(matrix, len(file_list), pos_ind, 1)\n",
        "\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ1CBrecjkVq"
      },
      "outputs": [],
      "source": [
        "# 5\n",
        "def top_relevant_doc(query_vector, tf_idf_matrix):\n",
        "  # rows = tf_idf_matrix.shape[0]\n",
        "  # print(rows)\n",
        "  # sim_docs = np.array([0.0]*num_docs)\n",
        "  # print(sim_docs)\n",
        "  score = 0\n",
        "  index = 0\n",
        "\n",
        "  # create dictionary to store key, value pairs: docId, similarity score\n",
        "  tf_idf_dict = {}\n",
        "\n",
        "  # loop through all rows in matrix to get dot product of documents with query vector\n",
        "\n",
        "  for doc in tf_idf_matrix:\n",
        "    score = np.dot(doc, query_vector)\n",
        "    tf_idf_dict[index+1] = score\n",
        "\n",
        "    index += 1\n",
        "\n",
        "  # sort documents by values\n",
        "  # sorted_docs = sorted(tf_idf_dict.items())\n",
        "\n",
        "  # sort documents based on calculated dot product value\n",
        "\n",
        "  sorted_docs = dict(sorted(tf_idf_dict.items(), key=lambda item: item[1],reverse = True))\n",
        "  # print(sorted_docs)\n",
        "\n",
        "  # top_five = sorted_docs[:5]\n",
        "  top_five = list(sorted_docs.items())[:5] #list of tuples\n",
        "\n",
        "  # for testing purposes\n",
        "  # for doc, score in top_five:\n",
        "  #     print(\"doc:\", doc)\n",
        "\n",
        "  return tf_idf_dict, top_five"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra0Lx-OGjkVq",
        "outputId": "0f5890ca-d5aa-43f4-b4d1-33c2824ae296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1 entered (cleaned):  ['hole']\n",
            "Weight selected:  2\n",
            "[(169, 97.22185083601492), (224, 19.97709263753731), (126, 18.64528646170149), (123, 17.313480285865673), (187, 17.313480285865673)]\n"
          ]
        }
      ],
      "source": [
        "# 6\n",
        "# Apply all 5 weighting schemes for term frequency calculation and provide the TF-IDF\n",
        "# score and results for each scheme individually\n",
        "\n",
        "# ask user to input sentence\n",
        "sentence = input(\"Enter your query (up to 5 words) : \" )\n",
        "chosen_weight = int(input(\"Choose weighting scheme (1, 2, 3, 4, 5): \" ))\n",
        "# calls the function at the beginning to clean the query\n",
        "cleaned_sentence = text_preprocessing_query(sentence)\n",
        "# prints the cleaned sentence so user can see what is happening\n",
        "print(\"Sentence 1 entered (cleaned): \" , cleaned_sentence)\n",
        "print(\"Weight selected: \", chosen_weight)\n",
        "\n",
        "index = 1\n",
        "weight_schemes = [1, 2, 3, 4, 5]\n",
        "# looping through in case user enters more than 5 words (more than 5 after the preprocessing) or does not enter a valid weight\n",
        "while len(cleaned_sentence) > 5 or chosen_weight not in weight_schemes:\n",
        "    index+= 1\n",
        "    sentence = input(\"Enter your query (up to 5 words) : \" )\n",
        "    chosen_weight = input(\"Choose weighting scheme (1, 2, 3, 4, 5): \" )\n",
        "    cleaned_sentence = text_preprocessing_query(sentence)\n",
        "    print(\"Sentence%2d entered (cleaned): \"% index, cleaned_sentence)\n",
        "    print(\"Weight selected: \", chosen_weight)\n",
        "\n",
        "query_vector ,vocab_list = create_qv(cleaned_sentence, pos_ind)\n",
        "matrix = tf_idf_matrix(len(file_list), pos_ind) #blank matix for the specific word\n",
        "result = tf_idf_vals(matrix, len(file_list), pos_ind, chosen_weight, vocab_list) # filled in matrix\n",
        "a, top5 = top_relevant_doc(query_vector, result)\n",
        "print(top5)\n",
        "\n",
        "print(\"Top 5 list (docID, score): \", top5)\n",
        "\n",
        "files_1 = []\n",
        "for i in top5:\n",
        "    files_1.append((file_list[i[0]], i[1]))\n",
        "\n",
        "print(\"Top 5 list (doc Name, score): \", files_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRHUmZoujkVq"
      },
      "source": [
        "### Question 3:  Cosine Similarity\n",
        "\n",
        "To calculate the Cosine Similarity Score for the query using each TF weighting scheme, and report the top 5 relevant documents for each scheme separately:\n",
        "\n",
        "1. Utilize the query and document vectors obtained from the TF-IDF Matrix in the previous question.\n",
        "\n",
        "2. Compute the Cosine Similarity Score for the query using each TF wighting scheme.\n",
        "\n",
        "3. Identify the top 5 relevant documents for each scheme separately based on the Cosine Similarity Score\n",
        "\n",
        "Note: for each scoring scheme, provide a report stating the pros and cons of using that particular scheme to determine document relevance\n",
        "\n",
        "Hint: Equation for cosine similarity can be represented as:\n",
        "\n",
        "\n",
        "Cosine Similarity = (A . B) / (||A|| * ||B||)\n",
        "\n",
        "where:\n",
        "\n",
        "- A . B represents the dot product of vectors A and B\n",
        "\n",
        "- ||A|| represents the Euclidean norm (magnitude) of vector A\n",
        "\n",
        "- ||B|| represents the Euclidean norm (magnitude) of vector B\n",
        "\n",
        "Using this equation, the cosine similarity score can be calculated for comparing the query vector adn the document vector of length of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDC4Bl67jkVq"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(query_vector, tf_idf_matrix):\n",
        "    dict_score = {}\n",
        "    # a represents the query\n",
        "    # b represents the document\n",
        "    a = query_vector \n",
        "    # print(a)\n",
        "    index = 1\n",
        "    for rows in tf_idf_matrix:\n",
        "        b= rows #tf-idf value for each document\n",
        "        formula = np.dot(a,b) / (norm(a) * norm(b))\n",
        "        dict_score[index] = formula\n",
        "        index+=1\n",
        "\n",
        "    # need to traverse through the list to get all the formulas and take top 5\n",
        "    sorted_docs_cs = dict(sorted(dict_score.items(), key=lambda item: item[1],reverse = True))\n",
        "    # print(sorted_docs)\n",
        "\n",
        "    # top_five = sorted_docs[:5]\n",
        "    top_five_cs = list(sorted_docs_cs.items())[:5] #list of tuples\n",
        "\n",
        "    return sorted_docs_cs, top_five_cs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG7nmUKtjkVq",
        "outputId": "e7878b3f-59ad-488f-c1b5-52a01c998e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 cosine list (docID, score):  [(1, 0.07946147677872809), (249, 0.07347550960730036), (175, 0.07037557407093438), (83, 0.05950549870129423), (116, 0.05731006230645433)]\n",
            "Top 5 cosine list (doc Name, score):  [('hole2nar.txt', 0.07946147677872809), ('tailbear.txt', 0.07347550960730036), ('aircon.txt', 0.07037557407093438), ('psf.txt', 0.05950549870129423), ('mydream.txt', 0.05731006230645433)]\n"
          ]
        }
      ],
      "source": [
        "sorted_cosine_doc,top_5_cosinesim = cosine_similarity(query_vector,result)\n",
        "print(\"Top 5 cosine list (docID, score): \", top_5_cosinesim)\n",
        "\n",
        "files = []\n",
        "for i in top_5_cosinesim:\n",
        "    files.append((file_list[i[0]], i[1]))\n",
        "\n",
        "print(\"Top 5 cosine list (doc Name, score): \", files)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
