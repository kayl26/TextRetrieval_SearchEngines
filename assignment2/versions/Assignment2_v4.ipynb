{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP423: Assignment 1\n",
    "\n",
    "Group Number: 5\n",
    "\n",
    "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
    "#pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "import os\n",
    "import re\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Positional Index\n",
    "\n",
    "a. Execute the subsequent preprocessing tasks on the provided dataset:\n",
    "\n",
    "1. Transform the text to lowercase\n",
    "\n",
    "2. Perform word tokenization\n",
    "\n",
    "3. Eliminate stopwords from the tokens\n",
    "\n",
    "4. Remove punctuation marks from the tokens\n",
    "\n",
    "5. Eliminate empty space tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_file(file):\n",
    "\n",
    "    # reading the file line by line\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # 1. converting to lowercase\n",
    "    lines = [x.lower() for x in lines] # list\n",
    "\n",
    "    # data = data.replace(\"-\", \" \")\n",
    "    # data = data.replace(\"'\", \" \")\n",
    "    # doing this here because the positional dictionary gets all the punctuation which then does not work\n",
    "    # 4. Remove punctuation / special characters\n",
    "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
    "    alphanumerictxt = []\n",
    "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
    "    # strips the empty string afterwards\n",
    "    for i in lines:\n",
    "        stripped = re.sub(pattern,\" \", i).strip()\n",
    "        split_stripped = stripped.split() # list\n",
    "        for j in split_stripped:\n",
    "            alphanumerictxt.append(j)\n",
    "\n",
    "     # join all the lines into one string\n",
    "    data = \" \".join(alphanumerictxt)\n",
    "\n",
    "    # 2. using the tokenization package to tokenize the words\n",
    "    tokenization = word_tokenize(data)\n",
    "    # print(tokenization)\n",
    "\n",
    "    # 3. remove any stop words from the NLTK package\n",
    "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
    "\n",
    "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
    "    stop_words = []\n",
    "    for word in tokenization:\n",
    "            if word not in stop_words_lib:\n",
    "                 stop_words.append(word)\n",
    "\n",
    "    # 5. eliminating any characters of length 1\n",
    "    updated_list = []\n",
    "\n",
    "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
    "    for i in stop_words:\n",
    "        if len(i)>1:\n",
    "            updated_list.append(i)\n",
    "    \n",
    "    # store the dictionary of positions / location for each word\n",
    "    dict_position = {} # dictionary has a word and the list of the position\n",
    "    position_counter = 1\n",
    "    # traverse through the list of all words \n",
    "    for word in updated_list:\n",
    "        # everytime it finds a new word it will create a new key and create a list for the position\n",
    "        if word not in dict_position: \n",
    "            dict_position[word] = [position_counter]\n",
    "        else: # the word already exists so just get the position it is found in\n",
    "\n",
    "             dict_position[word].append(position_counter)\n",
    "        position_counter+=1\n",
    "\n",
    "    #print(dict_position[\"by\"])\n",
    "\n",
    "    \n",
    "    # gets the positional index for the final list\n",
    "    dict_final = {}\n",
    "    for element in dict_position:\n",
    "        # print(\"Element: \", element)\n",
    "        dict_final[element] = dict_position[element]\n",
    "\n",
    "    return dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_query(query):\n",
    "\n",
    "    # a. converting to lowercase\n",
    "    lines = [x.lower() for x in query]\n",
    "\n",
    "    # join all the lines into one string\n",
    "    data = \"\".join(lines)\n",
    "\n",
    "    # b. using the tokenization package to tokenize the words\n",
    "    tokenization = word_tokenize(data)\n",
    "\n",
    "    # remove duplicates - less words to do the next steps with\n",
    "    newtoken = list(dict.fromkeys(tokenization))\n",
    "\n",
    "    # c. remove any stop words from the NLTK package\n",
    "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
    "\n",
    "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
    "    stop_words = []\n",
    "    for word in newtoken:\n",
    "            if word not in stop_words_lib:\n",
    "                 stop_words.append(word)\n",
    "\n",
    "    # print(stop_words)\n",
    "\n",
    "    # d. Remove special characters\n",
    "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
    "    alphanumerictxt = []\n",
    "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
    "    # strips the empty string afterwards\n",
    "    for i in stop_words:\n",
    "        alphanumerictxt.append(re.sub(pattern,\" \", i).strip())\n",
    "\n",
    "    # e. eliminating any characters of length 1\n",
    "    updated_list = []\n",
    "\n",
    "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
    "    for i in alphanumerictxt:\n",
    "        if len(i)>1:\n",
    "            updated_list.append(i)\n",
    "\n",
    "    # after all the other steps are completed check again to make sure no duplicated values\n",
    "    final_list = list(dict.fromkeys(updated_list))\n",
    "\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data files\n",
    "def readfile_fun():\n",
    "  # takes the names of the files in the folder and creates a list\n",
    "  file_names = os.listdir(\"./data\")\n",
    "\n",
    "  # file id (key)\n",
    "  counter = 1\n",
    "\n",
    "  #dictionary1 - gets the list of words, frequency and position\n",
    "  # i.e : to : 4 [1,3,5,7]\n",
    "  dict_words = {}\n",
    "  #dictionary2 - file dictionary (id to file name)\n",
    "  dict_files = {}\n",
    "\n",
    "  # goes through each file in the list of file names\n",
    "  for i in file_names:\n",
    "      \n",
    "    if counter <= 100: #for checking purposes - can remove before submitting\n",
    "        # gets the working directory (this may be different for everyone)\n",
    "        file_path = os.getcwd() + \"/data/\" + i\n",
    "        # opens the file in reading mode\n",
    "        file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
    "        # calls the function \n",
    "        # words is a dictionary\n",
    "        words = text_preprocessing_file(file)\n",
    "    \n",
    "        # updates the dictionaries\n",
    "        dict_words[counter] = words # dictionary of dictionary\n",
    "        dict_files[counter] = i # list of all the file names\n",
    "    \n",
    "        # increment the counter\n",
    "        counter+=1\n",
    "    \n",
    "        # closes the file\n",
    "        file.close()\n",
    "\n",
    "  return dict_words, dict_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Positional Index\n",
    "b. Develop the positional index data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. \n",
    "class Node:\n",
    "    # value: docID, count and list of position\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "# linked list for the document id\n",
    "class LinkedList:\n",
    "\n",
    "    # initialize linked list \n",
    "    def __init__(self): \n",
    "        self._front = None\n",
    "        self._rear = None\n",
    "        self._counter = 0\n",
    "\n",
    "    # key is the word value is the docID\n",
    "    # insert to theh beginning \n",
    "    def insert_front(self, value):\n",
    "        # if it is an empty list\n",
    "        newnode = Node(value)\n",
    "        if self._front is None:\n",
    "            self._front = newnode\n",
    "            self._rear = newnode\n",
    "        else:\n",
    "            newnode.next = self._front\n",
    "            self._front = newnode\n",
    "        self._counter +=1\n",
    "        return\n",
    "    \n",
    "    def insert_rear(self, value):\n",
    "        # if it is an empty list\n",
    "        newnode = Node(value)\n",
    "        if self._front is None:\n",
    "            self._front = newnode\n",
    "            self._rear = newnode\n",
    "        else:\n",
    "            self._rear.next = newnode\n",
    "            self._rear = newnode\n",
    "        self._counter +=1\n",
    "        return\n",
    "    \n",
    "    # for testing purposes - shows us the string representation of the linked list\n",
    "    def printList(self):\n",
    "        temp = self._front\n",
    "        while temp is not None:\n",
    "            print(temp.value)\n",
    "            temp = temp.next\n",
    "        return\n",
    "    \n",
    "    def merge(self):\n",
    "        merged_list = []\n",
    "        temp = self._front\n",
    "        while temp is not None:\n",
    "            merged_list.append(temp.value)\n",
    "            temp = temp.next\n",
    "        return merged_list\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        temp = self._front\n",
    "        newstring = \"\"\n",
    "        while temp is not None:\n",
    "            newstring += \" DocID: \"+ str(temp.value[0])\n",
    "            newstring += \", Frequency: \"+ str(temp.value[1])\n",
    "            newstring += \", Position: \"+ str(temp.value[2])+ \"\\n\"\n",
    "            temp = temp.next\n",
    "        return newstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position index - dictionary\n",
    "class PositionIndex:\n",
    "    def __init__(self):\n",
    "        self.dict_post = {} # key = word, value = linked list\n",
    "        \n",
    "    # key = word\n",
    "    def insert(self, key, docID, position):\n",
    "        newlist = [docID, len(position), position]\n",
    "        if key not in self.dict_post:\n",
    "            self.dict_post[key] = LinkedList() # created a new linked list for a new key\n",
    "        self.dict_post[key].insert_rear(newlist) # adds to the existing list\n",
    "        return \n",
    "\n",
    "    # takes one word and returns all the documents and positions        \n",
    "    def retrieve(self, key): # take the words and returns the entire linked list --> if key is found then returns entire linked list\n",
    "        ret = []\n",
    "        if key in self.dict_post:\n",
    "            ret = self.dict_post[key].merge()\n",
    "        return ret\n",
    "    \n",
    "    def retrieve_doc(self, key, docID):\n",
    "        #takes a list of docIDs and checks if that key is there - goes to the linked list and returns all the linked list of only the doc ids that match (sub linked list in that word)\n",
    "        doc_list = []\n",
    "        if key in self.dict_post:\n",
    "            word_list = self.dict_post[key].merge() # has the doc ids and positions\n",
    "            for doc in word_list: \n",
    "                if doc[0] == docID: # if the document id is the same\n",
    "                    doc_list.append(doc) # get the list\n",
    "        return doc_list\n",
    "\n",
    "    # stringify function\n",
    "    def __str__(self) -> str:\n",
    "        newstring = \"\"\n",
    "        for key in self.dict_post.keys():\n",
    "            newstring+= \"{ \" + key\n",
    "            newstring+= \": (\"+ str(self.dict_post[key]._counter) + \" {\\n\"\n",
    "            newstring+= str(self.dict_post[key])\n",
    "            newstring+= \"} ) } \\n\\n\"\n",
    "\n",
    "        return newstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashInvertedIndex:\n",
    "\n",
    "    # initialize hash table\n",
    "    def __init__(self, capacity=100): # manually set the capacity\n",
    "        self.capacity = capacity\n",
    "        self.table = [[] for _ in range(capacity)]\n",
    "        self.count = 0\n",
    "\n",
    "    # finding the corresponding value for the key\n",
    "    def _index(self, word):\n",
    "        index = hash(word) % self.capacity\n",
    "        return index\n",
    "\n",
    "    # add key-value pair to the hash table\n",
    "    def insert(self, key, value):\n",
    "\n",
    "        # compute the hash index for the word\n",
    "        hash_index = self._index(key)\n",
    "        for i, kv in enumerate(self.table[hash_index]):\n",
    "            k, v = kv\n",
    "            if k == key:\n",
    "                # add the value to the existing set of values\n",
    "                v.add(value)\n",
    "                # add the entry to the table\n",
    "                self.table[hash_index][i] = (k,v)\n",
    "                return\n",
    "        self.table[hash_index].append((key, {value}))\n",
    "\n",
    "    # retrieve the set of values for a given key, if not found, returns an empty set\n",
    "    def retrieve(self, key):\n",
    "        # compute the hash index for a given key\n",
    "        hash_index = self._index(key)\n",
    "        for k, v in self.table[hash_index]:\n",
    "            # if the key if found, return the set\n",
    "            if k == key:\n",
    "                return v\n",
    "        return set()\n",
    "\n",
    "    # for testing purposes - shows us the string representation of the hash tables\n",
    "    def __rep__(self):\n",
    "        return str(self.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the positional index - reads the documents and their corresponding words\n",
    "def create_positional_inverted_index():\n",
    "\n",
    "    # initialize the  linked list /inverted index\n",
    "    positional_index = PositionIndex()\n",
    "    # inverted_index = HashInvertedIndex()\n",
    "\n",
    "    # calls the function above\n",
    "    # returns list of words in doc and list of docs\n",
    "    words, files = readfile_fun()\n",
    "\n",
    "    # fill in the inverted index\n",
    "    # key = file IDs\n",
    "    # value = dictionary of words\n",
    "    for fileID, value in words.items():\n",
    "        for word in value.keys():\n",
    "            positional_index.insert(word, fileID, value[word]) # inserting counter instead of doc name\n",
    "            \n",
    "    return positional_index, files, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the inverted index - reads the documents and their corresponding words\n",
    "def create_inverted_index():\n",
    "\n",
    "    # initialize the  linked list /inverted index\n",
    "    inverted_index = HashInvertedIndex()\n",
    "\n",
    "    # calls the function above\n",
    "    # returns list of words in doc and list of docs\n",
    "    words, files = readfile_fun()\n",
    "    # fill in the inverted index\n",
    "    for key, value in words.items():\n",
    "        for word in value.keys():\n",
    "            inverted_index.insert(word, key) # inserting counter instead of doc name\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Positional Index\n",
    "\n",
    "c. Facilitate the searching of phrase queries, assuming the query length is equal to or less than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\n",
    "# takes query thats correct and give answer\n",
    "def find_common_docs(query, InvertedIndex):\n",
    "  index = 0\n",
    "  # print(query)\n",
    "  word = query[index]\n",
    "  doc_list = InvertedIndex.retrieve(word)\n",
    "  # FIND THE SHORTEST LIST\n",
    "  #loop through array of operations\n",
    "  while (index < len(query)-1):\n",
    "  # set the next word in the sentence and retrieve the inverted index for that using the retrieve function from the class in Q2\n",
    "    next_word = query[index+1]\n",
    "    list1 = InvertedIndex.retrieve(next_word)\n",
    "    doc_list = doc_list.intersection(list1)\n",
    "    index+=1\n",
    "\n",
    "  return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing in a list which includes all the words in the query, docIDs, frequency and position\n",
    "# position_words = dictionary, dictionary, linked list\n",
    "def find_positions(query, pos_ind, common_doc):\n",
    "    # create new_dictionary to store the words and the position dictionary\n",
    "    new_dict={}\n",
    "    new_list = [] # list of all the positions\n",
    "    adj_list = [] # positions are adjusted for the query\n",
    "    count = len(query)\n",
    "    i = 0\n",
    "\n",
    "    # get the common documents and positions\n",
    "    for word in query:\n",
    "        new_dict[word] = pos_ind.retrieve_doc(word, common_doc)\n",
    "        # words with only common document with the positions\n",
    "    # print(new_dict)\n",
    "    for word in query:\n",
    "        new_list.append(new_dict[word][0][2]) # pull out the positional list (at index 2)\n",
    "        # print(word)\n",
    "    # print(new_list)\n",
    "\n",
    "    # shifts the positions of the words to get them the same\n",
    "    for i in range (count):\n",
    "        another_list= new_list[i] # for the first word get the positions\n",
    "        temp_list = [x + count - 1 - i for x in another_list] # for every element in the list going to add the specific value and put into a new list\n",
    "        adj_list.append(temp_list)\n",
    "    # print(adj_list)\n",
    "    # gets the intersection\n",
    "    i = 0\n",
    "    intersected_list = adj_list[i]\n",
    "    i+=1\n",
    "    for i in range (count):\n",
    "        next_list = adj_list[i]\n",
    "        intersected_list = set(intersected_list).intersection(next_list)\n",
    "    # print(intersected_list)\n",
    "    \n",
    "    #unshift the positions\n",
    "    final_list = [x - (count - 1) for x in intersected_list]\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Positional Index\n",
    "\n",
    "Calling everything from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ind,file_list,position_words = create_positional_inverted_index()\n",
    "inverted_index = create_inverted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'startrek.txt', 2: 'vaincrow.txt', 3: 'mcdonaldl.txt', 4: 'sunday.txt', 5: 'snow.txt', 6: 'glimpse1.txt', 7: 'unluckwr.txt', 8: 'hareporc.txt', 9: 'cooldark.txt', 10: 'pepdegener.txt', 11: 'bagelman.txt', 12: 'fred.txt', 13: 'bulnoopt.txt', 14: 'dtruck.txt', 15: 'bulprint.txt', 16: 'vampword.txt', 17: 'lrrhood.txt', 18: 'sleprncs.txt', 19: 'lmermaid.txt', 20: 'berternie.txt', 21: 'mindprob.txt', 22: 'graymare.txt', 23: 'bulphrek.txt', 24: 'snowqn1.txt', 25: 'horswolf.txt', 26: 'alissadl.txt', 27: 'enya_trn.txt', 28: 'jackbstl.txt', 29: 'sanpedr2.txt', 30: 'gloves.txt', 31: 'plescopm.txt', 32: 'wolflamb.txt', 33: 'terrorbears.txt', 34: 'adler.txt', 35: 'antcrick.txt', 36: 'telefone.txt', 37: 'omarsheh.txt', 38: 'mydream.txt', 39: 'kneeslapper.txt', 40: 'mario.txt', 41: 'dskool.txt', 42: 'retrib.txt', 43: 'foxngrap.txt', 44: 'goldgoos.txt', 45: 'cabin.txt', 46: 'bulmrx.txt', 47: 'hellmach.txt', 48: 'knuckle.txt', 49: 'empnclot.txt', 50: 'traitor.txt', 51: 'sight.txt', 52: 'buldream.txt', 53: 'life.txt', 54: 'alad10.txt', 55: 'bruce-p.txt', 56: 'reality.txt', 57: 'aircon.txt', 58: 'wlgirl.txt', 59: 'cybersla.txt', 60: 'history5.txt', 61: '100west.txt', 62: 'melissa.txt', 63: 'elveshoe.txt', 64: 'tctac.txt', 65: 'ab40thv.txt', 66: 'musibrem.txt', 67: 'fish.txt', 68: 'keepmodu.txt', 69: 'poem-1.txt', 70: 'aislesix.txt', 71: 'flktrp.txt', 72: 'wolfcran.txt', 73: 'mattress.txt', 74: 'goldfish.txt', 75: 'parotsha.txt', 76: 'hansgrtl.txt', 77: 'socialvikings.txt', 78: 'aesop11.txt', 79: 'shoscomb.txt', 80: '3student.txt', 81: 'empty.txt', 82: 'blackp.txt', 83: 'spiders.txt', 84: 'goldenp.txt', 85: 'pinocch.txt', 86: 'oxfrog.txt', 87: 'obstgoat.txt', 88: 'lionmosq.txt', 89: 'mtinder.txt', 90: 'dopedenn.txt', 91: 'buggy.txt', 92: '7oldsamr.txt', 93: 'roger1.txt', 94: 'bulolli1.txt', 95: 'wolf7kid.txt', 96: 'veiledl.txt', 97: 'hotline1.txt', 98: 'lionmane.txt', 99: 'pussboot.txt', 100: 'bulironb.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query (up to 5 words) :  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 entered (cleaned):  ['hello']\n",
      "Document name:  startrek.txt\n",
      "The query start positions:  [8, 10]\n",
      "Document name:  cooldark.txt\n",
      "The query start positions:  [1257, 1253]\n",
      "Document name:  bagelman.txt\n",
      "The query start positions:  [306]\n",
      "Document name:  socialvikings.txt\n",
      "The query start positions:  [233, 237]\n",
      "Document name:  hellmach.txt\n",
      "The query start positions:  [3397]\n",
      "Document name:  bulphrek.txt\n",
      "The query start positions:  [1440, 3322]\n",
      "Document name:  cybersla.txt\n",
      "The query start positions:  [36828, 15669, 2006, 1183]\n",
      "Number of matching documents:  7\n",
      "File name(s):  ['startrek.txt', 'cooldark.txt', 'bagelman.txt', 'socialvikings.txt', 'hellmach.txt', 'bulphrek.txt', 'cybersla.txt']\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Enter your query (up to 5 words) : \" )\n",
    "# get every element of the word\n",
    "#stripped_sentence = sentence.split(\" \")\n",
    "# print(\"Sentence 1 entered: \", stripped_sentence)\n",
    "cleaned_sentence = text_preprocessing_query(sentence)\n",
    "print(\"Sentence 1 entered (cleaned): \" , cleaned_sentence) \n",
    "index = 1\n",
    "while len(cleaned_sentence) > 5:\n",
    "    index+= 1\n",
    "    sentence = input(\"Enter your query (up to 5 words) : \" )\n",
    "    cleaned_sentence = text_preprocessing_query(sentence)\n",
    "    print(\"Sentence%2d entered (cleaned): \"% index, cleaned_sentence)\n",
    "\n",
    "\n",
    "file_names = []\n",
    "if len(cleaned_sentence) > 0:\n",
    "    # finds all common documents\n",
    "    common_documents = find_common_docs(cleaned_sentence, inverted_index)\n",
    "    common_documents = list(common_documents)\n",
    "    count = 0\n",
    "    for doc in common_documents:\n",
    "        final = find_positions(cleaned_sentence, pos_ind, doc) # positions \n",
    "        if len(final)!= 0:\n",
    "            file_names.append(file_list[doc])\n",
    "            print(\"Document name: \",file_list[doc])\n",
    "            print(\"The query start positions: \", final)\n",
    "            count+=1\n",
    "    print(\"Number of matching documents: \", count)\n",
    "    print(\"File name(s): \",file_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Ranking and Term-Weighting\n",
    "\n",
    "1. First, apply the preprocessing steps mentioned in Q1 to the given dataset:\n",
    "- Convert the text to lowercase.\n",
    "- Perform word tokenization.\n",
    "- Remove stopwords from the tokens.\n",
    "- Remove punctuation marks from the tokens.\n",
    "- Remove blank space tokens.\n",
    "2. Construct the matrix with dimensions (number of documents) x (vocabulary size).\n",
    "3. Populate the TF-IDF values in the matrix for each word in the vocabulary.\n",
    "4. Create a query vector with a size equal to the vocabulary.\n",
    "5. Calculate the TF-IDF score for the query using the TF-IDF matrix. Identify the top\n",
    "5 relevant documents based on the score.\n",
    "6. Apply all 5 weighting schemes for term frequency calculation and provide the TF-\n",
    "IDF score and results for each scheme individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "#HAVE TO LIMIT ROWS AND COLS, WILL NOT RUN ON GOOGLE COLLAB\n",
    "# creates an empty TF-IDF matrix\n",
    "\n",
    "def tf_idf_matrix(doc_num, pos_ind, weight):\n",
    "  rows = 5 #doc_num\n",
    "  cols = 4 #len(pos_ind.dict_post)\n",
    "  matrix = np.empty((rows, cols), dtype = object).reshape(rows, cols)\n",
    "\n",
    "  return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "# populates TF-IDF Matrix with words in the vocbulary list\n",
    "\n",
    "def tf_idf_vals(matrix, doc_num, post_ind):\n",
    "  index = 0\n",
    "\n",
    "  for item in post_ind.dict_post:\n",
    "    if(index < 5): # cannot run whole thing, google collab crashes\n",
    "      matrix[index][0] = item\n",
    "      index += 1\n",
    "\n",
    "  return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting Schemes\n",
    "def weighting(pos_ind, docId, word, weight):\n",
    "  tf = 0\n",
    "\n",
    "  # 1) Binary, if word exists in documents returns 1, else 0\n",
    "  if weight == 1:\n",
    "    found = pos_ind.retrieve(word) # finds list of documents that has word\n",
    "    if len(found) == 0:\n",
    "      tf = 0\n",
    "    else:\n",
    "      tf = 1\n",
    "\n",
    "  # 2) Raw Count, getting the term frequency for the given word in the given document\n",
    "  elif weight == 2:\n",
    "    raw = pos_ind.retrieve(word)\n",
    "    for item in raw:\n",
    "      if item[0] == docId:\n",
    "        tf = item[1]\n",
    "        break;\n",
    "\n",
    "  # 3) Term Frequency, dividing raw count by compiment of chosen term\n",
    "  elif weight == 3:\n",
    "    found = pos_ind.retrieve(word)\n",
    "    if len(found) == 0:\n",
    "      tf = 0\n",
    "    \n",
    "    # frequency of word (chosen term)\n",
    "    else:\n",
    "      for item in found:\n",
    "        if item[0] == docId:\n",
    "          raw_count = item[1]\n",
    "          break;\n",
    "\n",
    "        # loop through position words dictionary to get list of words in a doc and count number of unique words\n",
    "        unique_words = 0\n",
    "        for key, value in position_words.items():\n",
    "            if key == docId:\n",
    "                for key, val in value.items():\n",
    "                    unique_words += 1\n",
    "                \n",
    "        print(unique_words) # for testing purposes\n",
    "\n",
    "        # compliment is the term frequency in the doc minus the word we are looking for\n",
    "        compliment = unique_words - 1\n",
    "        tf = raw_count/compliment\n",
    "    \n",
    "\n",
    "  # 4) Log Normalization, getting the term frequency for the given word and the given document, calling log function on that plus one\n",
    "  elif weight == 4:\n",
    "    raw = pos_ind.retrieve(word)\n",
    "    for item in raw:\n",
    "      if item[0] == docId:\n",
    "        raw_count = item[1]\n",
    "        break;\n",
    "          \n",
    "    tf = math.log(1 + raw_count)\n",
    "    \n",
    "  # 5) Double Normalization\n",
    "  elif weight == 5:\n",
    "    \n",
    "    # if word does not exist in doc\n",
    "    found = pos_ind.retrieve(word)\n",
    "    if len(found) == 0:\n",
    "      tf = 0\n",
    "\n",
    "    else:\n",
    "        # get raw count\n",
    "        raw = pos_ind.retrieve(word)\n",
    "        for item in raw:\n",
    "          if item[0] == docId:\n",
    "            raw_count = item[1]\n",
    "            break;\n",
    "\n",
    "        # loop through position words dictionary to get max term frequency\n",
    "        for key, value in position_words.items():\n",
    "            if key == docId:\n",
    "                for key, val in value.items():\n",
    "                    if key != word:\n",
    "                        # print(value.items())\n",
    "                        max = 0\n",
    "                        if len(val) > max:\n",
    "                            max = len(val)\n",
    "                        # print(key, len(val))\n",
    "        \n",
    "        tf = 0.5 + (0.5 * (raw_count/max))\n",
    "            \n",
    "        \n",
    "  return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "# testing Weighting Scheme function\n",
    "word = \"hello\"\n",
    "weight = 5\n",
    "docId = 1\n",
    "test = weighting(pos_ind, docId, word, weight)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 \n",
    "#Create query vector\n",
    "def create_qv(query, positional_index):\n",
    "\n",
    "    # create an empty query vector\n",
    "    query_vector = np.empty(len(positional_index.dict_post))\n",
    "\n",
    "    index = 0\n",
    "    for word in positional_index.dict_post:\n",
    "        if word in query:\n",
    "            query_vector[index] = 1\n",
    "        else:\n",
    "            query_vector[index] = 0\n",
    "        index += 1\n",
    "        \n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing query vector\n",
    "#print(pos_ind.dict_post)\n",
    "create_qv(\"good name\", pos_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_ind.retrieve(word))) #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Calculate TF-IDF score for the query using TF-IDF matrix\n",
    "# Identify top 5 relevant documents based on the score\n",
    "\n",
    "# TF - IDF = tf * IDF\n",
    "\n",
    "def calc_idf(pos_ind, docId, word, weight):\n",
    "    \n",
    "    # IDF(word) = log(total number of documents/ (document frequency(word) + 1))\n",
    "    idf = math.log(len(file_list) / (len(pos_ind.retrieve(word)) + 1))\n",
    "    \n",
    "    # TF - IDF = tf * IDF\n",
    "    tf_idf = idf * weigthing(pos_ind, docId, word, weight)\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate matrix with TF-IDF values\n",
    "\n",
    "tf_idf_matrix(doc_num, pos_ind, tf_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None None None None]\n",
      " [None None None None]\n",
      " [None None None None]\n",
      " [None None None None]\n",
      " [None None None None]]\n"
     ]
    }
   ],
   "source": [
    "# testing building empty matrix\n",
    "matrix = tf_idf_matrix(len(file_list), pos_ind, 1)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good' None None None]\n",
      " ['morning' None None None]\n",
      " ['name' None None None]\n",
      " ['mister' None None None]\n",
      " ['spock' None None None]]\n"
     ]
    }
   ],
   "source": [
    "# testing populating matrix with vocabulary words\n",
    "\n",
    "vals = tf_idf_vals(matrix, len(file_list), pos_ind)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
