{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP423: Assignment 1\n",
    "\n",
    "Group Number: 5\n",
    "\n",
    "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
    "#pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kayleighhabib/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kayleighhabib/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Positional Index\n",
    "\n",
    "a. Execute the subsequent preprocessing tasks on the provided dataset:\n",
    "\n",
    "1. Transform the text to lowercase\n",
    "\n",
    "2. Perform word tokenization\n",
    "\n",
    "3. Eliminate stopwords from the tokens\n",
    "\n",
    "4. Remove punctuation marks from the tokens\n",
    "\n",
    "5. Eliminate empty space tokens\n",
    "\n",
    "\n",
    "b. Develop the positional index data structure\n",
    "\n",
    "c. Facilitate the searching of phrase queries, assuming the query length is equal to or less than 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_str(query):\n",
    "\n",
    "    # 1. converting to lowercase\n",
    "    lines = [x.lower() for x in query] # list\n",
    "\n",
    "    # data = data.replace(\"-\", \" \")\n",
    "    # data = data.replace(\"'\", \" \")\n",
    "    # doing this here because the positional dictionary gets all the punctuation which then does not work\n",
    "    # 4. Remove punctuation / special characters\n",
    "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
    "    alphanumerictxt = []\n",
    "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
    "    # strips the empty string afterwards\n",
    "    for i in lines:\n",
    "        stripped = re.sub(pattern,\" \", i).strip()\n",
    "        split_stripped = stripped.split() # list\n",
    "        for j in split_stripped:\n",
    "            alphanumerictxt.append(j)\n",
    "\n",
    "     # join all the lines into one string\n",
    "    data = \" \".join(alphanumerictxt)\n",
    "\n",
    "    # 2. using the tokenization package to tokenize the words\n",
    "    tokenization = word_tokenize(data)\n",
    "    # print(tokenization)\n",
    "\n",
    "    # store the dictionary of positions / location for each word\n",
    "    dict_position = {} # dictionary has a word and the list of the position\n",
    "    position_counter = 1\n",
    "    # traverse through the list of all words \n",
    "    for word in tokenization:\n",
    "        # everytime it finds a new word it will create a new key and create a list for the position\n",
    "        if word not in dict_position: \n",
    "            dict_position[word] = [position_counter]\n",
    "        else: # the word already exists so just get the position it is found in\n",
    "\n",
    "             dict_position[word].append(position_counter)\n",
    "        position_counter+=1\n",
    "\n",
    "    #print(dict_position[\"by\"])\n",
    "\n",
    "    # 3. remove any stop words from the NLTK package\n",
    "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
    "\n",
    "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
    "    stop_words = []\n",
    "    for word in tokenization:\n",
    "            if word not in stop_words_lib:\n",
    "                 stop_words.append(word)\n",
    "\n",
    "    # 5. eliminating any characters of length 1\n",
    "    updated_list = []\n",
    "\n",
    "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
    "    for i in stop_words:\n",
    "        if len(i)>1:\n",
    "            updated_list.append(i)\n",
    "    \n",
    "    # gets the positional index for the final list\n",
    "    dict_final = {}\n",
    "    for element in updated_list:\n",
    "        # print(\"Element: \", element)\n",
    "        dict_final[element] = dict_position[element]\n",
    "\n",
    "    return dict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_file(file):\n",
    "\n",
    "    # reading the file line by line\n",
    "    lines = file.readlines()\n",
    "\n",
    "    # calls function above with the lines from the file\n",
    "    final_list = text_preprocessing_str(lines)\n",
    "\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data files\n",
    "def readfile_fun():\n",
    "  # takes the names of the files in the folder and creates a list\n",
    "  file_names = os.listdir(\"./data\")\n",
    "\n",
    "  # file id (key)\n",
    "  counter = 1\n",
    "\n",
    "  #dictionary1 - gets the list of words, frequency and position\n",
    "  # i.e : to : 4 [1,3,5,7]\n",
    "  dict_words = {}\n",
    "  #dictionary2 - file dictionary (id to file name)\n",
    "  dict_files = {}\n",
    "\n",
    "  # goes through each file in the list of file names\n",
    "  for i in file_names:\n",
    "    # gets the working directory (this may be different for everyone)\n",
    "    file_path = os.getcwd() + \"/data/\" + i\n",
    "    # opens the file in reading mode\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
    "    # calls the function \n",
    "    # words is a dictionary\n",
    "    words = text_preprocessing_file(file)\n",
    "\n",
    "    # updates the dictionaries\n",
    "    dict_words[counter] = words # dictionary of dictionary\n",
    "    dict_files[counter] = i # list of all the file names\n",
    "\n",
    "    # increment the counter\n",
    "    counter+=1\n",
    "\n",
    "    # closes the file\n",
    "    file.close()\n",
    "\n",
    "  return dict_words, dict_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words, files = readfile_fun()\n",
    "# # NEED TO SORT THE POSTING LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. \n",
    "class Node:\n",
    "    # value: docID, count and list of position\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.next = None\n",
    "\n",
    "# linked list for the document id\n",
    "class LinkedList:\n",
    "\n",
    "    # initialize linked list \n",
    "    def __init__(self): \n",
    "        self._front = None\n",
    "        self._rear = None\n",
    "        self._counter = 0\n",
    "\n",
    "    # key is the word value is the docID\n",
    "    # insert to theh beginning \n",
    "    def insert_front(self, value):\n",
    "        # if it is an empty list\n",
    "        newnode = Node(value)\n",
    "        if self._front is None:\n",
    "            self._front = newnode\n",
    "            self._rear = newnode\n",
    "        else:\n",
    "            newnode.next = self._front\n",
    "            self._front = newnode\n",
    "        self._counter +=1\n",
    "        return\n",
    "    \n",
    "    def insert_rear(self, value):\n",
    "        # if it is an empty list\n",
    "        newnode = Node(value)\n",
    "        if self._front is None:\n",
    "            self._front = newnode\n",
    "            self._rear = newnode\n",
    "        else:\n",
    "            self._rear.next = newnode\n",
    "            self._rear = newnode\n",
    "        self._counter +=1\n",
    "        return\n",
    "    \n",
    "    # for testing purposes - shows us the string representation of the linked list\n",
    "    def printList(self):\n",
    "        temp = self._front\n",
    "        while temp is not None:\n",
    "            print(temp.value)\n",
    "            temp = temp.next\n",
    "        return\n",
    "    \n",
    "    def merge(self):\n",
    "        merged_list = []\n",
    "        temp = self._front\n",
    "        while temp is not None:\n",
    "            merged_list.append(temp.value)\n",
    "            temp = temp.next\n",
    "        return merged_list\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        temp = self._front\n",
    "        newstring = \"\"\n",
    "        while temp is not None:\n",
    "            newstring += \" DocID: \"+ str(temp.value[0])\n",
    "            newstring += \", Frequency: \"+ str(temp.value[1])\n",
    "            newstring += \", Position: \"+ str(temp.value[2])+ \"\\n\"\n",
    "            temp = temp.next\n",
    "        return newstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position index - dictionary\n",
    "class PositionIndex:\n",
    "    def __init__(self):\n",
    "        self.dict_post = {} # key = word, value = linked list\n",
    "\n",
    "    # key = word\n",
    "    def insert(self, key, docID, position):\n",
    "        newlist = [docID, len(position), position]\n",
    "        if key not in self.dict_post:\n",
    "            self.dict_post[key] = LinkedList() # created a new linked list for a new key\n",
    "        self.dict_post[key].insert_rear(newlist) # adds to the existing list\n",
    "        return \n",
    "\n",
    "    # takes one word and returns all the documents and positions        \n",
    "    def retrieve(self, key):\n",
    "        ret = []\n",
    "        if key in self.dict_post:\n",
    "            ret = self.dict_post[key].merge()\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    # stringify function\n",
    "    def __str__(self) -> str:\n",
    "        newstring = \"\"\n",
    "        for key in self.dict_post.keys():\n",
    "            newstring+= \"{ \" + key\n",
    "            newstring+= \": (\"+ str(self.dict_post[key]._counter) + \" {\\n\"\n",
    "            newstring+= str(self.dict_post[key])\n",
    "            newstring+= \"} ) } \\n\\n\"\n",
    "\n",
    "        return newstring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the positional index - reads the documents and their corresponding words\n",
    "def create_inverted_index():\n",
    "\n",
    "    # initialize the  linked list /inverted index\n",
    "    positional_index = PositionIndex()\n",
    "\n",
    "    # calls the function above\n",
    "    # returns list of words in doc and list of docs\n",
    "    words, files = readfile_fun()\n",
    "\n",
    "    # fill in the inverted index\n",
    "    # key = file IDs\n",
    "    # value = dictionary of words\n",
    "    for fileID, value in words.items():\n",
    "        for word in value.keys():\n",
    "            positional_index.insert(word, fileID, value[word]) # inserting counter instead of doc name\n",
    "\n",
    "    return positional_index, files, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ind,file,word = create_inverted_index()\n",
    "\n",
    "# print(pos_ind)\n",
    "\n",
    "# print(file)\n",
    "# print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. \n",
    "# takes query thats correct and give answer\n",
    "def search_queries(query, positional_index):\n",
    "    searched = positional_index.retrieve(query)\n",
    "\n",
    "    return searched\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, [2]], [3, 1, [1587]], [4, 2, [9956, 32717]]]\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Enter your query: \" )\n",
    "found = search_queries(sentence, pos_ind)\n",
    "print(found)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
