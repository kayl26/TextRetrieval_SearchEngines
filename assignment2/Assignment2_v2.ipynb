{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK5_wdZZpxkl"
      },
      "source": [
        "# CP423: Assignment 1\n",
        "\n",
        "Group Number: 5\n",
        "\n",
        "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HsaZL_upxkn"
      },
      "outputs": [],
      "source": [
        "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
        "#pip install certifi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC8v-vzApxko"
      },
      "outputs": [],
      "source": [
        "import certifi\n",
        "import ssl\n",
        "import os\n",
        "\n",
        "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA0bnGAzpxkp",
        "outputId": "fa20199b-78ec-4104-8bc5-d84704e0f810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# inputs\n",
        "import os\n",
        "import re\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_B5UXeVpxkr"
      },
      "source": [
        "### Question 1: Positional Index\n",
        "\n",
        "a. Execute the subsequent preprocessing tasks on the provided dataset:\n",
        "\n",
        "1. Transform the text to lowercase\n",
        "\n",
        "2. Perform word tokenization\n",
        "\n",
        "3. Eliminate stopwords from the tokens\n",
        "\n",
        "4. Remove punctuation marks from the tokens\n",
        "\n",
        "5. Eliminate empty space tokens\n",
        "\n",
        "\n",
        "b. Develop the positional index data structure\n",
        "\n",
        "c. Facilitate the searching of phrase queries, assuming the query length is equal to or less than 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIgWcFRLpxkr"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing_str(query):\n",
        "\n",
        "    # 1. converting to lowercase\n",
        "    lines = [x.lower() for x in query] # list\n",
        "\n",
        "    # data = data.replace(\"-\", \" \")\n",
        "    # data = data.replace(\"'\", \" \")\n",
        "    # doing this here because the positional dictionary gets all the punctuation which then does not work\n",
        "    # 4. Remove punctuation / special characters\n",
        "    pattern = \"[^a-z0-9]\" # pattern NOT with any letter between a-z or any number including 0-9 (could also do A-Z but since we already made everything lower don't need to add that)\n",
        "    alphanumerictxt = []\n",
        "    # goes through each index in the list and if it matches the pattern it will put an empty string otherwise will preserve the value\n",
        "    # strips the empty string afterwards\n",
        "    for i in lines:\n",
        "        stripped = re.sub(pattern,\" \", i).strip()\n",
        "        split_stripped = stripped.split() # list\n",
        "        for j in split_stripped:\n",
        "            alphanumerictxt.append(j)\n",
        "\n",
        "     # join all the lines into one string\n",
        "    data = \" \".join(alphanumerictxt)\n",
        "\n",
        "    # 2. using the tokenization package to tokenize the words\n",
        "    tokenization = word_tokenize(data)\n",
        "    # print(tokenization)\n",
        "\n",
        "    # store the dictionary of positions / location for each word\n",
        "    dict_position = {} # dictionary has a word and the list of the position\n",
        "    position_counter = 1\n",
        "    # traverse through the list of all words\n",
        "    for word in tokenization:\n",
        "        # everytime it finds a new word it will create a new key and create a list for the position\n",
        "        if word not in dict_position:\n",
        "            dict_position[word] = [position_counter]\n",
        "        else: # the word already exists so just get the position it is found in\n",
        "\n",
        "             dict_position[word].append(position_counter)\n",
        "        position_counter+=1\n",
        "\n",
        "    #print(dict_position[\"by\"])\n",
        "\n",
        "    # 3. remove any stop words from the NLTK package\n",
        "    stop_words_lib = set(stopwords.words(\"english\")) # choosing the english stopwords from the package\n",
        "\n",
        "    # goes through each word, and checks if it is in the library - if it is not then we keep the word, if it is we don't take it\n",
        "    stop_words = []\n",
        "    for word in tokenization:\n",
        "            if word not in stop_words_lib:\n",
        "                 stop_words.append(word)\n",
        "\n",
        "    # 5. eliminating any characters of length 1\n",
        "    updated_list = []\n",
        "\n",
        "    # goes through each index in the list and if the length is more than 1 will append to new list\n",
        "    for i in stop_words:\n",
        "        if len(i)>1:\n",
        "            updated_list.append(i)\n",
        "\n",
        "    # gets the positional index for the final list\n",
        "    dict_final = {}\n",
        "    for element in updated_list:\n",
        "        # print(\"Element: \", element)\n",
        "        dict_final[element] = dict_position[element]\n",
        "\n",
        "    return dict_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPVgf-Tmpxks"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing_file(file):\n",
        "\n",
        "    # reading the file line by line\n",
        "    lines = file.readlines()\n",
        "\n",
        "    # calls function above with the lines from the file\n",
        "    final_list = text_preprocessing_str(lines)\n",
        "\n",
        "    return final_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf6uUp8Lpxks"
      },
      "outputs": [],
      "source": [
        "# import data files\n",
        "def readfile_fun():\n",
        "  # takes the names of the files in the folder and creates a list\n",
        "  file_names = os.listdir(\"./data\")\n",
        "\n",
        "  # file id (key)\n",
        "  counter = 1\n",
        "\n",
        "  #dictionary1 - gets the list of words, frequency and position\n",
        "  # i.e : to : 4 [1,3,5,7]\n",
        "  dict_words = {}\n",
        "  #dictionary2 - file dictionary (id to file name)\n",
        "  dict_files = {}\n",
        "\n",
        "  # goes through each file in the list of file names\n",
        "  for i in file_names:\n",
        "    # gets the working directory (this may be different for everyone)\n",
        "    file_path = os.getcwd() + \"/data/\" + i\n",
        "    # opens the file in reading mode\n",
        "    file = open(file_path, \"r\", encoding=\"utf-8\", errors = \"ignore\" ) # added the encoding and errors as some files had different encoding detectors (note: with new line can add new param --> newline = '/r/n'))\n",
        "    # calls the function\n",
        "    # words is a dictionary\n",
        "    words = text_preprocessing_file(file)\n",
        "\n",
        "    # updates the dictionaries\n",
        "    dict_words[counter] = words # dictionary of dictionary\n",
        "    dict_files[counter] = i # list of all the file names\n",
        "\n",
        "    # increment the counter\n",
        "    counter+=1\n",
        "\n",
        "    # closes the file\n",
        "    file.close()\n",
        "\n",
        "  return dict_words, dict_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-SuZK0kpxkt"
      },
      "outputs": [],
      "source": [
        "# words, files = readfile_fun()\n",
        "# # NEED TO SORT THE POSTING LISTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40eAeiztpxkt"
      },
      "outputs": [],
      "source": [
        "#b.\n",
        "class Node:\n",
        "    # value: docID, count and list of position\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.next = None\n",
        "\n",
        "# linked list for the document id\n",
        "class LinkedList:\n",
        "\n",
        "    # initialize linked list\n",
        "    def __init__(self):\n",
        "        self._front = None\n",
        "        self._rear = None\n",
        "        self._counter = 0\n",
        "\n",
        "    # key is the word value is the docID\n",
        "    # insert to theh beginning\n",
        "    def insert_front(self, value):\n",
        "        # if it is an empty list\n",
        "        newnode = Node(value)\n",
        "        if self._front is None:\n",
        "            self._front = newnode\n",
        "            self._rear = newnode\n",
        "        else:\n",
        "            newnode.next = self._front\n",
        "            self._front = newnode\n",
        "        self._counter +=1\n",
        "        return\n",
        "\n",
        "    def insert_rear(self, value):\n",
        "        # if it is an empty list\n",
        "        newnode = Node(value)\n",
        "        if self._front is None:\n",
        "            self._front = newnode\n",
        "            self._rear = newnode\n",
        "        else:\n",
        "            self._rear.next = newnode\n",
        "            self._rear = newnode\n",
        "        self._counter +=1\n",
        "        return\n",
        "\n",
        "    # for testing purposes - shows us the string representation of the linked list\n",
        "    def printList(self):\n",
        "        temp = self._front\n",
        "        while temp is not None:\n",
        "            print(temp.value)\n",
        "            temp = temp.next\n",
        "        return\n",
        "\n",
        "    def merge(self):\n",
        "        merged_list = []\n",
        "        temp = self._front\n",
        "        while temp is not None:\n",
        "            merged_list.append(temp.value)\n",
        "            temp = temp.next\n",
        "        return merged_list\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        temp = self._front\n",
        "        newstring = \"\"\n",
        "        while temp is not None:\n",
        "            newstring += \" DocID: \"+ str(temp.value[0])\n",
        "            newstring += \", Frequency: \"+ str(temp.value[1])\n",
        "            newstring += \", Position: \"+ str(temp.value[2])+ \"\\n\"\n",
        "            temp = temp.next\n",
        "        return newstring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RX4LPvzpxkt"
      },
      "outputs": [],
      "source": [
        "# position index - dictionary\n",
        "class PositionIndex:\n",
        "    def __init__(self):\n",
        "        self.dict_post = {} # key = word, value = linked list\n",
        "\n",
        "    # key = word\n",
        "    def insert(self, key, docID, position):\n",
        "        newlist = [docID, len(position), position] # len(position) is term frequency\n",
        "        if key not in self.dict_post:\n",
        "            self.dict_post[key] = LinkedList() # created a new linked list for a new key\n",
        "        self.dict_post[key].insert_rear(newlist) # adds to the existing list\n",
        "        return\n",
        "\n",
        "    # takes one word and returns all the documents and positions\n",
        "    def retrieve(self, key):\n",
        "        ret = []\n",
        "        if key in self.dict_post:\n",
        "            ret = self.dict_post[key].merge()\n",
        "\n",
        "        return ret\n",
        "\n",
        "    # stringify function\n",
        "    def __str__(self) -> str:\n",
        "        newstring = \"\"\n",
        "        for key in self.dict_post.keys():\n",
        "            newstring+= \"{ \" + key\n",
        "            newstring+= \": (\"+ str(self.dict_post[key]._counter) + \" {\\n\"\n",
        "            newstring+= str(self.dict_post[key])\n",
        "            newstring+= \"} ) } \\n\\n\"\n",
        "\n",
        "        return newstring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eZ2kyS8pxku"
      },
      "outputs": [],
      "source": [
        "# build the positional index - reads the documents and their corresponding words\n",
        "def create_inverted_index():\n",
        "\n",
        "    # initialize the  linked list /inverted index\n",
        "    positional_index = PositionIndex()\n",
        "\n",
        "    # calls the function above\n",
        "    # returns list of words in doc and list of docs\n",
        "    words, files = readfile_fun()\n",
        "\n",
        "    # fill in the inverted index\n",
        "    # key = file IDs\n",
        "    # value = dictionary of words\n",
        "    for fileID, value in words.items():\n",
        "        for word in value.keys():\n",
        "            positional_index.insert(word, fileID, value[word]) # inserting counter instead of doc name\n",
        "\n",
        "    return positional_index, files, words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEmwAVVGpxku",
        "outputId": "455b4f00-7b3e-460c-dac2-cb4bd1b3f0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249\n"
          ]
        }
      ],
      "source": [
        "pos_ind,file,word = create_inverted_index()\n",
        "\n",
        "# print(pos_ind)\n",
        "\n",
        "# print(file)\n",
        "# print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQzZm09cpxkv"
      },
      "outputs": [],
      "source": [
        "#c.\n",
        "# takes query thats correct and give answer\n",
        "def search_queries(query, positional_index):\n",
        "    searched = positional_index.retrieve(query)\n",
        "\n",
        "    return searched\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Djrl-d7zpxkw",
        "outputId": "2871d614-3ba6-4e1d-e091-a4a1d2cdfa06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: castle\n",
            "[[2, 1, [18138]], [41, 10, [143, 215, 592, 631, 706, 988, 1102, 1207, 1382, 1387]], [45, 5, [731, 733, 838, 856, 883]], [72, 1, [162]], [74, 2, [194, 262]], [82, 6, [990, 1054, 1413, 1549, 1728, 1769]], [84, 5, [360, 456, 558, 600, 1077]], [91, 10, [149, 463, 511, 573, 734, 750, 762, 828, 980, 1020]], [105, 1, [21072]], [107, 7, [176, 2537, 4520, 4733, 4795, 4821, 5366]], [140, 13, [2574, 6653, 13647, 18299, 21760, 22356, 26924, 26934, 29054, 34231, 39869, 40021, 40110]], [144, 1, [13460]], [147, 2, [2122, 2135]], [163, 9, [27, 104, 194, 226, 324, 497, 623, 919, 1255]], [164, 2, [1482, 1574]], [186, 8, [237, 259, 279, 562, 862, 966, 1264, 1320]], [200, 11, [212, 308, 450, 714, 730, 744, 804, 984, 1008, 1066, 1123]], [209, 6, [905, 1139, 1157, 1488, 1673, 1720]], [245, 3, [8854, 14070, 16552]]]\n"
          ]
        }
      ],
      "source": [
        "sentence = input(\"Enter your query: \" )\n",
        "found = search_queries(sentence, pos_ind)\n",
        "print(found)\n",
        "# print(pos_ind)\n",
        "#doc id, term frequency, list of positions within doc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2: Ranking and Term-Weighting\n",
        "\n",
        "1. First, apply the preprocessing steps mentioned in Q1 to the given dataset:\n",
        "- Convert the text to lowercase.\n",
        "- Perform word tokenization.\n",
        "- Remove stopwords from the tokens.\n",
        "- Remove punctuation marks from the tokens.\n",
        "- Remove blank space tokens.\n",
        "2. Construct the matrix with dimensions (number of documents) x (vocabulary size).\n",
        "3. Populate the TF-IDF values in the matrix for each word in the vocabulary.\n",
        "4. Create a query vector with a size equal to the vocabulary.\n",
        "5. Calculate the TF-IDF score for the query using the TF-IDF matrix. Identify the top\n",
        "5 relevant documents based on the score.\n",
        "6. Apply all 5 weighting schemes for term frequency calculation and provide the TF-\n",
        "IDF score and results for each scheme individually.\n"
      ],
      "metadata": {
        "id": "eKd3PJFcr1xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "\n",
        "#HAVE TO LIMIT ROWS AND COLS, WILL NOT RUN ON GOOGLE COLLAB\n",
        "\n",
        "# creates an empty TF-IDF matrix\n",
        "\n",
        "def tf_idf_matrix(doc_num, pos_ind):\n",
        "  rows = 5 #doc_num\n",
        "  cols = 4 #len(pos_ind.dict_post)\n",
        "  matrix = np.empty((rows, cols), dtype = object).reshape(rows, cols)\n",
        "\n",
        "  return matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "m0FGn4yyr_Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "# populates TF-IDF Matrix with words in the vocbulary list\n",
        "\n",
        "def tf_idf_vals(matrix, doc_num, post_ind):\n",
        "  index = 0\n",
        "\n",
        "  for item in post_ind.dict_post:\n",
        "    if(index < 5): # cannot run whole thing, google collab crashes\n",
        "      matrix[index][0] = item\n",
        "      index += 1\n",
        "\n",
        "  return matrix\n"
      ],
      "metadata": {
        "id": "jVWAx6YI5KEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighting Schemes\n",
        "\n",
        "def weighting(pos_ind, docId, word, weight):\n",
        "  tf = 0\n",
        "\n",
        "  # Binary, if word exists in documents returns 1, else 0\n",
        "\n",
        "  if weight == 1:\n",
        "    found = pos_ind.retrieve(word)\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "\n",
        "    else:\n",
        "      tf = 1\n",
        "\n",
        "  # Raw Count, getting the term frequency for the given word in the given document\n",
        "\n",
        "  if weight == 2:\n",
        "    raw = pos_ind.retrieve(word)\n",
        "\n",
        "    for item in raw:\n",
        "      if item[0] == docId:\n",
        "        tf = item[1]\n",
        "        break;\n",
        "\n",
        "  # Term Frequency, diving raw count by ...????\n",
        "\n",
        "  if weight == 3:\n",
        "\n",
        "    found = pos_ind.retrieve(word)\n",
        "\n",
        "    if len(found) == 0:\n",
        "      tf = 0\n",
        "\n",
        "    else:\n",
        "      for item in found:\n",
        "        if item[0] == docId:\n",
        "          raw_count = item[1]\n",
        "          break;\n",
        "\n",
        "      compliment = len(pos_ind.dict_post) - 1 # this is not correct, need to get the term freq for a single doc\n",
        "\n",
        "      tf = raw_count/compliment\n",
        "\n",
        "  # Log Normalization, getting the term frequency for the given word and the given document, calling log function on that plus one\n",
        "\n",
        "  if weight == 4:\n",
        "    raw = pos_ind.retrieve(word)\n",
        "\n",
        "    for item in raw:\n",
        "      if item[0] == docId:\n",
        "        raw_count = item[1]\n",
        "        break;\n",
        "\n",
        "    tf = math.log(1 + raw_count)\n",
        "\n",
        "\n",
        "  # need to figure out weight == 3 for this...\n",
        "\n",
        "  # if weight == 5:\n",
        "\n",
        "\n",
        "  return tf\n"
      ],
      "metadata": {
        "id": "4r6EtT2aF0yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing Weighting Scheme function\n",
        "\n",
        "word = \"castle\"\n",
        "weight = 3\n",
        "docId = 41\n",
        "test = weighting(pos_ind, docId, word, weight)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "sYuyTupMHb0c",
        "outputId": "3e1d687b-f778-4adf-fbc5-7b0cf2b5a2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'int' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-205-884038ed40fb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m41\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-203-82b3f67110ca>\u001b[0m in \u001b[0;36mweighting\u001b[0;34m(pos_ind, docId, word, weight)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdocId\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0mraw_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m           \u001b[0mcompliment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing building empty matrix\n",
        "\n",
        "matrix = tf_idf_matrix(len(file), pos_ind)\n",
        "print(matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK5lXVdI2SDI",
        "outputId": "917e7b1c-ebb8-4ac9-b794-a68f8e4fae04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[None None None None]\n",
            " [None None None None]\n",
            " [None None None None]\n",
            " [None None None None]\n",
            " [None None None None]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing populating matrix with vocabulary words\n",
        "\n",
        "vals = tf_idf_vals(matrix, len(file), pos_ind)\n",
        "print(vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIE2YB0o8ZpH",
        "outputId": "9da78939-3d64-41b7-e1ef-b18d0d8e707f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['last' None None None]\n",
            " ['night' None None None]\n",
            " ['went' None None None]\n",
            " ['bed' None None None]\n",
            " ['feeling' None None None]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6VTrCa18dwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}