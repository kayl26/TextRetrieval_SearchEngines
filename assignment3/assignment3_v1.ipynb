{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP423: Assignment 3\n",
    "\n",
    "Group Number: 5\n",
    "\n",
    "Group Members: Abigail Lee (200469770), Kayleigh Habib (200370580) and Myisha Chaudhry (200591740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIGHT NEED TO RUN THIS FOR THE NLTK\n",
    "#pip install certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import ssl\n",
    "import os\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 : Introduction to Webscraping \n",
    "\n",
    "Your task involves working with the given URL, which pertains to historical population counts of various Canadian provinces as recorded on Wikipedia \n",
    "(https://en.wikipedia.org/wiki/List_of_Canadian_provinces_and_territories_by_historical_population). \n",
    "\n",
    "Your goal in this segment is to import HTML tables from the web and manipulate them using pandas.\n",
    "\n",
    "\n",
    "Here's a breakdown of what you will do:\n",
    "\n",
    "1. Retrieve the specified webpage as raw HTML using the requests library\n",
    "\n",
    "2. Decode the HTML into a tree-structured Python object with the BeautifulSoup library\n",
    "\n",
    "3. Utilize BeautifulSoup to identify and extract only the tables we're interested in\n",
    "\n",
    "4. Merge the tables, sanitize the text, and transform them into a single Python dictionary\n",
    "\n",
    "5. Construct a pandas dataframe out of this dictionary\n",
    "\n",
    "6. Locate all h2 elements on the HTML page and display their text content\n",
    "\n",
    "7. Generate a list of all the hyperlinks embedded within the tables\n",
    "\n",
    "8. Download every webpage by traversing the links included in the list created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Retrieve the specified webpage as raw HTML\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Canadian_provinces_and_territories_by_historical_population\"\n",
    "response = requests.get(url)\n",
    "raw_html = response.text\n",
    "# raw_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Decode the HTML into a tree-structure object\n",
    "tree_stru = BeautifulSoup(raw_html, features = \"html.parser\")\n",
    "# tree_stru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Identify and extract only tables we are interested in\n",
    "inter_tables = tree_stru.find_all('table',class_='wikitable')\n",
    "# interest_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m inter_tables: \u001b[38;5;66;03m# go through all the tables in the above section\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# merging the tables together\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m table:\n\u001b[1;32m      6\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[38;5;28mstr\u001b[39m(i), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# Part 4: Merge, Sanatize and transform into Python dictionary\n",
    "dictionary = {}\n",
    "for table in inter_tables: # go through all the tables in the above section\n",
    "    dictionary.union(table) # merging the tables together\n",
    "    for i in table:\n",
    "        soup = BeautifulSoup(str(i), 'html.parser').get_text()\n",
    "        dictionary[i]=soup.to_dict()\n",
    "\n",
    "dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Construct a pandas dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
